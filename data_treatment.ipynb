{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Initial setup: downloads and dataset loading"
      ],
      "metadata": {
        "id": "u8VKPZeAHc4T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "# --- CONFIGURAZIONE PERCORSI GLOBALI ---\n",
        "DRIVE_FOLDER = \"/content/drive/MyDrive/Esame_Complexity\"\n",
        "\n",
        "# Creazione cartella se non esiste\n",
        "if not os.path.exists(DRIVE_FOLDER):\n",
        "    try:\n",
        "        os.makedirs(DRIVE_FOLDER)\n",
        "    except:\n",
        "        # Questo serve nel caso in cui il Drive non sia ancora montato\n",
        "        pass\n",
        "\n",
        "datasets = {\n",
        "    'ose_adv_ele': 'OSE_adv_ele.csv',\n",
        "    'ose_adv_int': 'OSE_adv_int.csv',\n",
        "    'swipe': 'swipe.csv',\n",
        "    'vikidia': 'vikidia.csv'\n",
        "}"
      ],
      "metadata": {
        "id": "mj9XFAdC9SaJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4-yg-UYrHTkf"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "!{sys.executable} -m pip install nltk\n",
        "!{sys.executable} -m pip install textcomplexity\n",
        "!{sys.executable} -m pip install stanza\n",
        "!{sys.executable} -m pip install wordfreq\n",
        "!{sys.executable} -m spacy download en_core_web_md\n",
        "!{sys.executable} -m pip install tqdm spacy numpy"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Standard library imports\n",
        "import json\n",
        "from collections import Counter\n",
        "from functools import lru_cache\n",
        "from pprint import pprint\n",
        "from typing import Dict, Set, Iterable, Optional, Any, Tuple\n",
        "import importlib.resources as pkg_resources\n",
        "\n",
        "# Third-party imports\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import nltk\n",
        "from nltk.corpus import wordnet as wn\n",
        "import spacy\n",
        "import stanza\n",
        "import textcomplexity  # only used to access en.json\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "# Download required resources\n",
        "stanza.download('en')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n",
        "\n",
        "# Make sure WordNet is available; if not, download it.\n",
        "try:\n",
        "    _ = wn.synsets(\"dog\")\n",
        "except LookupError:\n",
        "    nltk.download(\"wordnet\")\n",
        "    nltk.download(\"omw-1.4\")\n",
        "\n",
        "# Load spaCy model\n",
        "nlp = spacy.load(\"en_core_web_md\", disable=[\"ner\", \"textcat\"])\n",
        "spacy_nlp = nlp\n",
        "spacy_nlp.add_pipe(\"sentencizer\")\n",
        "\n",
        "\n",
        "# Stanza pipeline cache\n",
        "@lru_cache(maxsize=None)  # Cache pipelines for different languages\n",
        "def get_stanza_pipeline(lang: str):\n",
        "    if lang == 'en':\n",
        "        # Use pre-trained models for English, including constituency parser\n",
        "        # for CS metric and dependency parser for MDD metric\n",
        "        return stanza.Pipeline(lang='en', processors='tokenize,pos,lemma,depparse,constituency')\n",
        "    else:\n",
        "        raise ValueError(f\"Unsupported language: {lang}\")\n",
        "\n",
        "# Define CONTENT_UPOS - this was missing previously, causing an error if not defined globally or locally where used\n",
        "# Based on the usage in _compute_lexical_density, these are Universal POS tags for content words\n",
        "CONTENT_UPOS = {\"NOUN\", \"PROPN\", \"ADJ\", \"VERB\", \"ADV\"}\n",
        "\n",
        "# Define CONTENT_POS - this was also missing previously and is used in the discourse complexity functions\n",
        "CONTENT_POS =  {\"NOUN\", \"VERB\", \"ADJ\", \"ADV\"}"
      ],
      "metadata": {
        "id": "7n6vDSBpIySA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "datasets ={'ose_adv_ele':'OSE_adv_ele.csv',\n",
        "           'ose_adv_int':'OSE_adv_int.csv',\n",
        "           'swipe': 'swipe.csv',\n",
        "           'vikidia':'vikidia.csv'}\n",
        "\n",
        "def load_data(path):\n",
        "    return pd.read_csv(path, sep='\\t')\n",
        "\n",
        "\n",
        "def load_dataset(name):\n",
        "    if name not in datasets:\n",
        "        raise ValueError(f\"Dataset {name} not found\")\n",
        "    return load_data(datasets[name])"
      ],
      "metadata": {
        "id": "kQI2wmufLfWh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = load_dataset('ose_adv_ele')\n",
        "df.head(3)"
      ],
      "metadata": {
        "id": "IMQerAw8Lq94"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "row = df.sample(1)\n",
        "\n",
        "print('SIMPLE TEXT')\n",
        "print(row['Simple'].iloc[0])\n",
        "print('-'*100)\n",
        "print('COMPLEX TEXT')\n",
        "print(row['Complex'].iloc[0])"
      ],
      "metadata": {
        "id": "SOPuZnSJWutm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cnt = 0\n",
        "for name, path in datasets.items():\n",
        "    df = load_dataset(name)\n",
        "    print(f\"{name}: {df.shape[0]} rows\")\n",
        "    cnt += df.shape[0]\n",
        "print(f\"Total: {cnt} rows\")"
      ],
      "metadata": {
        "id": "-L5cy__2W5Zb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = load_dataset('ose_adv_ele')"
      ],
      "metadata": {
        "id": "5LMFFYWFXBhA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cache stanza pipelines to avoid re-loading models\n",
        "_STANZA_PIPELINES: Dict[str, stanza.Pipeline] = {}\n",
        "\n",
        "# UPOS tags considered content words (C)\n",
        "CONTENT_UPOS = {\"NOUN\", \"PROPN\", \"VERB\", \"ADJ\", \"ADV\"}\n",
        "\n",
        "\n",
        "@lru_cache()\n",
        "def load_cow_top5000_en() -> Set[str]:\n",
        "    \"\"\"\n",
        "    Load the COW-based list of the 5,000 most frequent English content words\n",
        "    from textcomplexity's English language definition file (en.json).\n",
        "\n",
        "    We ignore POS tags and keep only lowercased word forms.\n",
        "    \"\"\"\n",
        "    with pkg_resources.files(textcomplexity).joinpath(\"en.json\").open(\n",
        "        \"r\", encoding=\"utf-8\"\n",
        "    ) as f:\n",
        "        lang_def = json.load(f)\n",
        "\n",
        "    most_common = lang_def[\"most_common\"]  # list of [word, xpos]\n",
        "    cow_top5000 = {w.lower() for w, xpos in most_common}\n",
        "    return cow_top5000\n",
        "\n",
        "\n",
        "def get_stanza_pipeline(lang: str = \"en\", use_gpu: bool = True) -> stanza.Pipeline:\n",
        "    \"\"\"\n",
        "    Get (or create) a cached stanza Pipeline for a given language.\n",
        "\n",
        "    NOTE: You must have downloaded the models beforehand, e.g.:\n",
        "        import stanza\n",
        "        stanza.download('en')\n",
        "    \"\"\"\n",
        "    if lang not in _STANZA_PIPELINES:\n",
        "        _STANZA_PIPELINES[lang] = stanza.Pipeline(\n",
        "            lang=lang,\n",
        "            processors=\"tokenize,pos,lemma,depparse,constituency\",\n",
        "            use_gpu=use_gpu,\n",
        "            tokenize_no_ssplit=False,\n",
        "        )\n",
        "    return _STANZA_PIPELINES[lang]"
      ],
      "metadata": {
        "id": "6umob97WpWAY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Lex. Complexity"
      ],
      "metadata": {
        "id": "rs2Ai6agYvIH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def _compute_mtld(tokens: Iterable[str], ttr_threshold: float = 0.72) -> Optional[float]:\n",
        "    \"\"\"\n",
        "    Compute MTLD (Measure of Textual Lexical Diversity) for a list of tokens.\n",
        "\n",
        "    MTLD = total_number_of_tokens / number_of_factors\n",
        "\n",
        "    A factor is a contiguous segment where the running TTR stays >= threshold.\n",
        "    When the TTR drops below the threshold, we close a factor (at the previous\n",
        "    token) and start a new one. At the end, the remaining partial segment is\n",
        "    counted as a fractional factor, with weight proportional to how close the\n",
        "    final TTR is to the threshold.\n",
        "    \"\"\"\n",
        "    tokens = [tok for tok in tokens if tok]\n",
        "    if not tokens:\n",
        "        return None\n",
        "\n",
        "    types = set()\n",
        "    factor_count = 0.0\n",
        "    token_count_in_factor = 0\n",
        "\n",
        "    for tok in tokens:\n",
        "        token_count_in_factor += 1\n",
        "        types.add(tok)\n",
        "        ttr = len(types) / token_count_in_factor\n",
        "\n",
        "        if ttr < ttr_threshold:\n",
        "            factor_count += 1.0\n",
        "            types = set()\n",
        "            token_count_in_factor = 0\n",
        "\n",
        "    # final partial factor\n",
        "    if token_count_in_factor > 0:\n",
        "        final_ttr = len(types) / token_count_in_factor\n",
        "        if final_ttr < 1.0:\n",
        "            fractional = (1.0 - final_ttr) / (1.0 - ttr_threshold)\n",
        "            fractional = max(0.0, min(1.0, fractional))\n",
        "            factor_count += fractional\n",
        "\n",
        "    if factor_count == 0:\n",
        "        return None\n",
        "\n",
        "    return len(tokens) / factor_count\n",
        "\n",
        "\n",
        "\n",
        "def _compute_lexical_density(total_tokens: int, content_tokens: int) -> Optional[float]:\n",
        "    \"\"\"\n",
        "    LD = |C| / |T|\n",
        "    where:\n",
        "        |C| = number of content-word tokens\n",
        "        |T| = total number of non-punctuation tokens\n",
        "    \"\"\"\n",
        "    if total_tokens == 0:\n",
        "        return None\n",
        "    return content_tokens / total_tokens\n",
        "\n",
        "\n",
        "def _compute_lexical_sophistication_cow(\n",
        "    content_forms: Iterable[str],\n",
        "    cow_top5000: set,\n",
        ") -> Optional[float]:\n",
        "    \"\"\"\n",
        "    LS = |{ w in C : w not in R }| / |C|\n",
        "    where:\n",
        "        C = content-word tokens (surface forms, lowercased)\n",
        "        R = COW top-5000 content word forms (lowercased)\n",
        "    \"\"\"\n",
        "    forms = [f for f in content_forms if f]\n",
        "    if not forms:\n",
        "        return None\n",
        "\n",
        "    off_list = sum(1 for f in forms if f not in cow_top5000)\n",
        "    return off_list / len(forms)\n",
        "\n",
        "\n",
        "@lru_cache(maxsize=1)\n",
        "def load_cow_top5000_en() -> Set[str]:\n",
        "    \"\"\"\n",
        "    Load the COW top-5000 English content word forms from textcomplexity package data.\n",
        "    The list is expected to be in 'textcomplexity/data/en.json' under the key 'cow_top5000'.\n",
        "    \"\"\"\n",
        "    # Use importlib.resources.files for modern package data access\n",
        "    json_path = pkg_resources.files('textcomplexity').joinpath('en.json')\n",
        "    with json_path.open('r') as f:\n",
        "        data = json.load(f)\n",
        "        return set(data.get(\"cow_top5000\", []))\n",
        "\n",
        "\n",
        "def lexical_measures_from_doc(doc) -> Dict[str, Optional[float]]:\n",
        "    \"\"\"\n",
        "    Compute MTLD, LD, LS from a stanza Document.\n",
        "    \"\"\"\n",
        "    cow_top5000 = load_cow_top5000_en()\n",
        "\n",
        "    mtld_tokens = []\n",
        "    total_tokens = 0\n",
        "    content_tokens = 0\n",
        "    content_forms = []\n",
        "\n",
        "    for sent in doc.sentences:\n",
        "        for word in sent.words:\n",
        "            if word.upos == \"PUNCT\":\n",
        "                continue\n",
        "\n",
        "            lemma = (word.lemma or word.text or \"\").lower()\n",
        "            if not lemma:\n",
        "                continue\n",
        "\n",
        "            mtld_tokens.append(lemma)\n",
        "            total_tokens += 1\n",
        "\n",
        "            if word.upos in CONTENT_UPOS:\n",
        "                content_tokens += 1\n",
        "                form = (word.text or \"\").lower()\n",
        "                content_forms.append(form)\n",
        "\n",
        "    mtld = _compute_mtld(mtld_tokens) if mtld_tokens else None\n",
        "    ld = _compute_lexical_density(total_tokens, content_tokens)\n",
        "    ls = _compute_lexical_sophistication_cow(content_forms, cow_top5000)\n",
        "\n",
        "    return {\"MTLD\": mtld, \"LD\": ld, \"LS\": ls}\n",
        "\n",
        "\n",
        "def lexical_measures_from_text(text: str, lang: str = \"en\") -> Dict[str, Optional[float]]:\n",
        "    \"\"\"\n",
        "    Convenience wrapper: parse a single text and compute lexical measures.\n",
        "    \"\"\"\n",
        "    if text is None:\n",
        "        text = \"\"\n",
        "    text = str(text)\n",
        "\n",
        "    if not text.strip():\n",
        "        return {\"MTLD\": None, \"LD\": None, \"LS\": None}\n",
        "\n",
        "    nlp = get_stanza_pipeline(lang)\n",
        "    doc = nlp(text)\n",
        "    return lexical_measures_from_doc(doc)\n",
        "\n",
        "\n",
        "\n",
        "def compute_lexical_measures_df(\n",
        "    df: pd.DataFrame,\n",
        "    column: str = \"text\",\n",
        "    lang: str = \"en\",\n",
        ") -> Dict[str, Dict[Any, Optional[float]]]:\n",
        "    \"\"\"\n",
        "    Compute lexical measures for each row in df[column].\n",
        "\n",
        "    Returns:\n",
        "        {\n",
        "            \"MTLD\": {index: value},\n",
        "            \"LD\":   {index: value},\n",
        "            \"LS\":   {index: value},\n",
        "        }\n",
        "    \"\"\"\n",
        "    mtld_res: Dict[Any, Optional[float]] = {}\n",
        "    ld_res: Dict[Any, Optional[float]] = {}\n",
        "    ls_res: Dict[Any, Optional[float]] = {}\n",
        "\n",
        "    for idx, text in df[column].items():\n",
        "        metrics = lexical_measures_from_text(text, lang=lang)\n",
        "        mtld_res[idx] = metrics[\"MTLD\"]\n",
        "        ld_res[idx] = metrics[\"LD\"]\n",
        "        ls_res[idx] = metrics[\"LS\"]\n",
        "\n",
        "    return {\"MTLD\": mtld_res, \"LD\": ld_res, \"LS\": ls_res}"
      ],
      "metadata": {
        "id": "CaO_NBbFYZwv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Synt. Complexity"
      ],
      "metadata": {
        "id": "Z0Jt7G4xYzAU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def mdd_from_doc(doc) -> Optional[float]:\n",
        "    \"\"\"\n",
        "    Compute Mean Dependency Distance (MDD) from a stanza Document.\n",
        "\n",
        "    For each sentence s_i with dependency set D_i:\n",
        "        MDD_i = (1 / |D_i|) * sum_{(h,d) in D_i} |h - d|\n",
        "    Then:\n",
        "        MDD = (1 / k) * sum_i MDD_i, over all sentences with at least one dependency.\n",
        "    \"\"\"\n",
        "    sentence_mdds = []\n",
        "\n",
        "    for sent in doc.sentences:\n",
        "        distances = []\n",
        "        for w in sent.words:\n",
        "            if w.head is None or w.head == 0:\n",
        "                continue\n",
        "            distances.append(abs(w.id - w.head))\n",
        "\n",
        "        if distances:\n",
        "            sentence_mdds.append(sum(distances) / len(distances))\n",
        "\n",
        "    if not sentence_mdds:\n",
        "        return None\n",
        "    return sum(sentence_mdds) / len(sentence_mdds)\n",
        "\n",
        "\n",
        "\n",
        "def _count_clauses_in_tree(tree) -> int:\n",
        "    \"\"\"\n",
        "    Count clause nodes in a constituency tree.\n",
        "\n",
        "    A simple and standard heuristic (PTB-style) is:\n",
        "        count all nodes whose label starts with 'S'\n",
        "        (S, SBAR, SBARQ, SINV, SQ, etc.).\n",
        "\n",
        "    This aligns with the idea of counting finite and subordinate clauses\n",
        "    as in Hunt (1965) and later complexity work.\n",
        "    \"\"\"\n",
        "    if tree is None:\n",
        "        return 0\n",
        "\n",
        "    # Stanza's constituency tree: tree.label, tree.children\n",
        "    count = 1 if getattr(tree, \"label\", \"\").startswith(\"S\") else 0\n",
        "\n",
        "    for child in getattr(tree, \"children\", []):\n",
        "        # leaves can be strings or terminals without 'label'\n",
        "        if hasattr(child, \"label\"):\n",
        "            count += _count_clauses_in_tree(child)\n",
        "\n",
        "    return count\n",
        "\n",
        "\n",
        "def cs_from_doc(doc) -> Optional[float]:\n",
        "    \"\"\"\n",
        "    Compute CS (clauses per sentence) from a stanza Document.\n",
        "\n",
        "        CS = (1 / k) * sum_i L_i\n",
        "\n",
        "    where L_i is the number of clauses in sentence s_i, estimated by counting\n",
        "    all constituents whose label starts with 'S' in the constituency tree of s_i.\n",
        "    \"\"\"\n",
        "    clause_counts = []\n",
        "    for sent in doc.sentences:\n",
        "        tree = getattr(sent, \"constituency\", None)\n",
        "        if tree is None:\n",
        "            # No constituency tree available for this sentence\n",
        "            continue\n",
        "        num_clauses = _count_clauses_in_tree(tree)\n",
        "        clause_counts.append(num_clauses)\n",
        "\n",
        "    if not clause_counts:\n",
        "        return None\n",
        "\n",
        "    return sum(clause_counts) / len(clause_counts)\n",
        "\n",
        "\n",
        "\n",
        "def syntactic_measures_from_doc(doc) -> Dict[str, Optional[float]]:\n",
        "    \"\"\"\n",
        "    Compute MDD and CS from a stanza Document.\n",
        "    \"\"\"\n",
        "    mdd = mdd_from_doc(doc)\n",
        "    cs = cs_from_doc(doc)\n",
        "    return {\"MDD\": mdd, \"CS\": cs}\n",
        "\n",
        "\n",
        "def syntactic_measures_from_text(text: str, lang: str = \"en\") -> Dict[str, Optional[float]]:\n",
        "    \"\"\"\n",
        "    Convenience wrapper: parse a single text and compute syntactic measures.\n",
        "    \"\"\"\n",
        "    if text is None:\n",
        "        text = \"\"\n",
        "    text = str(text)\n",
        "\n",
        "    if not text.strip():\n",
        "        return {\"MDD\": None, \"CS\": None}\n",
        "\n",
        "    nlp = get_stanza_pipeline(lang)\n",
        "    doc = nlp(text)\n",
        "    return syntactic_measures_from_doc(doc)\n",
        "\n",
        "\n",
        "def compute_syntactic_measures_df(\n",
        "    df: pd.DataFrame,\n",
        "    column: str = \"text\",\n",
        "    lang: str = \"en\",\n",
        ") -> Dict[str, Dict[Any, Optional[float]]]:\n",
        "    \"\"\"\n",
        "    Compute syntactic measures for each row in df[column].\n",
        "\n",
        "    Returns:\n",
        "        {\n",
        "            \"MDD\": {index: value},\n",
        "            \"CS\":  {index: value},\n",
        "        }\n",
        "    \"\"\"\n",
        "    mdd_res: Dict[Any, Optional[float]] = {}\n",
        "    cs_res: Dict[Any, Optional[float]] = {}\n",
        "\n",
        "    for idx, text in df[column].items():\n",
        "        metrics = syntactic_measures_from_text(text, lang=lang)\n",
        "        mdd_res[idx] = metrics[\"MDD\"]\n",
        "        cs_res[idx] = metrics[\"CS\"]\n",
        "\n",
        "    return {\"MDD\": mdd_res, \"CS\": cs_res}"
      ],
      "metadata": {
        "id": "GU7peLloY7kR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Discourse complexity"
      ],
      "metadata": {
        "id": "D6dMlvuAZKvE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Approximate set of content POS tags (spaCy universal POS)\n",
        "CONTENT_POS =  {\"NOUN\", \"VERB\", \"ADJ\", \"ADV\"}\n",
        "\n",
        "\n",
        "def is_content_token(tok):\n",
        "    \"\"\"\n",
        "    Return True if token is considered a content word.\n",
        "    We ignore stopwords, punctuation, and non-alphabetic tokens.\n",
        "    \"\"\"\n",
        "    return (\n",
        "        tok.is_alpha\n",
        "        and not tok.is_stop\n",
        "        and tok.pos_ in CONTENT_POS\n",
        "    )\n",
        "\n",
        "\n",
        "@lru_cache(maxsize=100000)\n",
        "def get_related_lemmas(lemma):\n",
        "    \"\"\"\n",
        "    Return a set of semantically related lemmas for the given lemma\n",
        "    using WordNet, including:\n",
        "      - synonyms\n",
        "      - antonyms\n",
        "      - hypernyms / hyponyms\n",
        "      - meronyms (part/member/substance)\n",
        "      - coordinate terms (siblings under the same hypernym)\n",
        "\n",
        "    NOTE: Some older examples mention 'troponyms', but in NLTK's\n",
        "    WordNet interface there is no 'troponyms()' method on Synset,\n",
        "    so we do NOT use it here.\n",
        "    \"\"\"\n",
        "    lemma = lemma.lower()\n",
        "    related = set()\n",
        "    synsets = wn.synsets(lemma)\n",
        "\n",
        "    for syn in synsets:\n",
        "        # Synonyms and antonyms\n",
        "        for l in syn.lemmas():\n",
        "            related.add(l.name().lower().replace(\"_\", \" \"))\n",
        "            for ant in l.antonyms():\n",
        "                related.add(ant.name().lower().replace(\"_\", \" \"))\n",
        "\n",
        "        # Hypernyms (more general) and hyponyms (more specific)\n",
        "        for hyper in syn.hypernyms():\n",
        "            for l in hyper.lemmas():\n",
        "                related.add(l.name().lower().replace(\"_\", \" \"))\n",
        "        for hypo in syn.hyponyms():\n",
        "            for l in hypo.lemmas():\n",
        "                related.add(l.name().lower().replace(\"_\", \" \"))\n",
        "\n",
        "        # Meronyms: part/member/substance\n",
        "        for mer in syn.part_meronyms() + syn.member_meronyms() + syn.substance_meronyms():\n",
        "            for l in mer.lemmas():\n",
        "                related.add(l.name().lower().replace(\"_\", \" \"))\n",
        "\n",
        "        # Coordinate terms (siblings under same hypernym)\n",
        "        for hyper in syn.hypernyms():\n",
        "            for sibling in hyper.hyponyms():\n",
        "                if sibling == syn:\n",
        "                    continue\n",
        "                for l in sibling.lemmas():\n",
        "                    related.add(l.name().lower().replace(\"_\", \" \"))\n",
        "\n",
        "    # Remove the lemma itself if present\n",
        "    related.discard(lemma)\n",
        "    return related\n",
        "\n",
        "\n",
        "def lexical_cohesion_single(text, nlp):\n",
        "    \"\"\"\n",
        "    Compute Lexical Cohesion (LC) for a single document:\n",
        "\n",
        "        LC = |C| / m\n",
        "\n",
        "    where:\n",
        "      - |C| is the number of cohesive devices between sentences\n",
        "        (lexical repetition + semantic relations),\n",
        "      - m  is the total number of word tokens (alphabetic) in the document.\n",
        "\n",
        "    If the document has fewer than 2 sentences or no valid words,\n",
        "    LC is returned as 0.0.\n",
        "    \"\"\"\n",
        "    if not isinstance(text, str) or not text.strip():\n",
        "        return 0.0\n",
        "\n",
        "    doc = nlp(text)\n",
        "\n",
        "    # Total number of alphabetic tokens (denominator m)\n",
        "    m = sum(1 for tok in doc if tok.is_alpha)\n",
        "    if m == 0:\n",
        "        return 0.0\n",
        "\n",
        "    sentences = list(doc.sents)\n",
        "    if len(sentences) < 2:\n",
        "        # With only one sentence, cross-sentence cohesion is not defined\n",
        "        return 0.0\n",
        "\n",
        "    # Collect sets of content lemmas per sentence\n",
        "    sent_lemmas = []\n",
        "    for sent in sentences:\n",
        "        lemmas = set(\n",
        "            tok.lemma_.lower()\n",
        "            for tok in sent\n",
        "            if is_content_token(tok)\n",
        "        )\n",
        "        if lemmas:\n",
        "            sent_lemmas.append(lemmas)\n",
        "\n",
        "    if len(sent_lemmas) < 2:\n",
        "        return 0.0\n",
        "\n",
        "    cohesive_count = 0\n",
        "\n",
        "    for i in range(len(sent_lemmas) - 1):\n",
        "        for j in range(i + 1, len(sent_lemmas)):\n",
        "            li = sent_lemmas[i]\n",
        "            lj = sent_lemmas[j]\n",
        "\n",
        "            # 1) Lexical repetition: shared lemmas\n",
        "            shared = li & lj\n",
        "            cohesive_count += len(shared)\n",
        "\n",
        "            # 2) Semantic relations via WordNet\n",
        "            for lemma in li:\n",
        "                related = get_related_lemmas(lemma)\n",
        "                cohesive_count += len(related & lj)\n",
        "\n",
        "    return float(cohesive_count) / float(m)\n",
        "\n",
        "\n",
        "def sentence_vector(sent, vector_size):\n",
        "    \"\"\"\n",
        "    Represent a sentence as the average of token vectors.\n",
        "    If no token has a vector, return a zero vector.\n",
        "    \"\"\"\n",
        "    vecs = [\n",
        "        tok.vector\n",
        "        for tok in sent\n",
        "        if tok.has_vector and not tok.is_punct and not tok.is_space\n",
        "    ]\n",
        "    if not vecs:\n",
        "        return np.zeros(vector_size, dtype=\"float32\")\n",
        "    return np.mean(vecs, axis=0)\n",
        "\n",
        "\n",
        "def coherence_single(text, nlp):\n",
        "    \"\"\"\n",
        "    Compute Coherence (CoH) for a single document as the average\n",
        "    cosine similarity between adjacent sentence vectors:\n",
        "\n",
        "        CoH = (1 / (k-1)) * sum_{i=1}^{k-1} cos(h_i, h_{i+1})\n",
        "\n",
        "    where h_i is the sentence/topic vector for sentence i.\n",
        "\n",
        "    If the document has fewer than 2 sentences, CoH = 0.0.\n",
        "    \"\"\"\n",
        "    if not isinstance(text, str) or not text.strip():\n",
        "        return 0.0\n",
        "\n",
        "    if nlp.vocab.vectors_length == 0:\n",
        "        raise ValueError(\n",
        "            \"The loaded spaCy model does not contain word vectors \"\n",
        "            \"(nlp.vocab.vectors_length == 0). \"\n",
        "            \"Use a model like 'en_core_web_md' or similar.\"\n",
        "        )\n",
        "\n",
        "    doc = nlp(text)\n",
        "    sentences = list(doc.sents)\n",
        "    k = len(sentences)\n",
        "\n",
        "    if k < 2:\n",
        "        # Only one sentence: no adjacent pair, coherence = 0.0\n",
        "        return 0.0\n",
        "\n",
        "    vector_size = nlp.vocab.vectors_length\n",
        "    sent_vectors = [\n",
        "        sentence_vector(sent, vector_size)\n",
        "        for sent in sentences\n",
        "    ]\n",
        "\n",
        "    sims = []\n",
        "    for i in range(k - 1):\n",
        "        v1 = sent_vectors[i]\n",
        "        v2 = sent_vectors[i + 1]\n",
        "        norm1 = np.linalg.norm(v1)\n",
        "        norm2 = np.linalg.norm(v2)\n",
        "        denom = norm1 * norm2\n",
        "        if denom == 0.0:\n",
        "            # Skip pairs where at least one sentence vector is zero\n",
        "            continue\n",
        "        cos_sim = float(np.dot(v1, v2) / denom)\n",
        "        sims.append(cos_sim)\n",
        "\n",
        "    if not sims:\n",
        "        return 0.0\n",
        "\n",
        "    return float(np.mean(sims))\n",
        "\n",
        "\n",
        "\n",
        "def compute_lexical_cohesion_vector(df, nlp, column=\"text\"):\n",
        "    \"\"\"\n",
        "    Compute LC for each row of a DataFrame.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    df : pandas.DataFrame\n",
        "        DataFrame containing the texts.\n",
        "    nlp : spaCy Language object\n",
        "        Pre-loaded spaCy pipeline with lemmatizer, POS tagger, etc.\n",
        "    column : str, default \"text\"\n",
        "        Name of the column that contains the text.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    np.ndarray\n",
        "        1D array of LC scores, length == len(df).\n",
        "    \"\"\"\n",
        "    texts = df[column].fillna(\"\").astype(str)\n",
        "    scores = [lexical_cohesion_single(t, nlp) for t in texts]\n",
        "    return np.array(scores, dtype=\"float32\")\n",
        "\n",
        "\n",
        "def compute_coherence_vector(df, nlp, column=\"text\"):\n",
        "    \"\"\"\n",
        "    Compute CoH for each row of a DataFrame.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    df : pandas.DataFrame\n",
        "        DataFrame containing the texts.\n",
        "    nlp : spaCy Language object\n",
        "        Pre-loaded spaCy pipeline with word vectors.\n",
        "    column : str, default \"text\"\n",
        "        Name of the column that contains the text.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    np.ndarray\n",
        "        1D array of CoH scores, length == len(df).\n",
        "    \"\"\"\n",
        "    texts = df[column].fillna(\"\").astype(str)\n",
        "    scores = [coherence_single(t, nlp) for t in texts]\n",
        "    return np.array(scores, dtype=\"float32\")\n",
        "\n",
        "\n",
        "def compute_discourse_measures(df, nlp, column=\"text\"):\n",
        "    \"\"\"\n",
        "    Compute both LC and CoH for each row of a DataFrame and return\n",
        "    them in a dictionary.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    dict\n",
        "        {\n",
        "            \"LC\":  np.ndarray of lexical cohesion scores,\n",
        "            \"CoH\": np.ndarray of coherence scores\n",
        "        }\n",
        "    \"\"\"\n",
        "    lc_vec = compute_lexical_cohesion_vector(df, nlp, column=column)\n",
        "    coh_vec = compute_coherence_vector(df, nlp, column=column)\n",
        "    return {\"LC\": lc_vec, \"CoH\": coh_vec}"
      ],
      "metadata": {
        "id": "KRqBF_4wZAE7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Calculate Complexity Values"
      ],
      "metadata": {
        "id": "rzhNKtUrZO2n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def _analyze_text_all(text: str, lang: str = \"en\") -> Dict[str, Optional[float]]:\n",
        "    \"\"\"\n",
        "    Parse a text with stanza and compute all measures (lexical + syntactic)\n",
        "    in a single pass.\n",
        "\n",
        "    Returns a dict with keys:\n",
        "        \"MTLD\", \"LD\", \"LS\", \"MDD\", \"CS\"\n",
        "    (Discourse measures LC/CoH are added later at DataFrame level, via spaCy.)\n",
        "    \"\"\"\n",
        "    if text is None:\n",
        "        text = \"\"\n",
        "    text = str(text)\n",
        "\n",
        "    if not text.strip():\n",
        "        return {\"MTLD\": None, \"LD\": None, \"LS\": None, \"MDD\": None, \"CS\": None}\n",
        "\n",
        "    nlp = get_stanza_pipeline(lang)\n",
        "    doc = nlp(text)\n",
        "\n",
        "    lex = lexical_measures_from_doc(doc)\n",
        "    syn = syntactic_measures_from_doc(doc)\n",
        "\n",
        "    out: Dict[str, Optional[float]] = {}\n",
        "    out.update(lex)\n",
        "    out.update(syn)\n",
        "    return out\n",
        "\n",
        "\n",
        "def compute_all_complexity_measures_df(\n",
        "    df: pd.DataFrame,\n",
        "    column: str = \"text\",\n",
        "    lang: str = \"en\",\n",
        "    spacy_nlp=None,\n",
        ") -> Dict[str, Dict[Any, Optional[float]]]:\n",
        "    \"\"\"\n",
        "    Compute all complexity measures for each row in df[column].\n",
        "\n",
        "    Args\n",
        "    ----\n",
        "    df : pandas.DataFrame\n",
        "        DataFrame with a text column.\n",
        "    column : str, default \"text\"\n",
        "        Name of the text column.\n",
        "    lang : str, default \"en\"\n",
        "        Language code for stanza.\n",
        "    n_jobs : int, default 1\n",
        "        Number of worker processes to use.\n",
        "            - 1  : sequential execution (no multiprocessing).\n",
        "            - >1 : multiprocessing with that many workers.\n",
        "            - 0 or None : use cpu_count() workers.\n",
        "    spacy_nlp : spaCy Language, required for LC / CoH\n",
        "        Pre-loaded spaCy pipeline with:\n",
        "            - POS / lemmatizer for LC\n",
        "            - word vectors for CoH (e.g. 'en_core_web_md').\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    dict\n",
        "        {\n",
        "            \"MTLD\": {index: value},\n",
        "            \"LD\":   {index: value},\n",
        "            \"LS\":   {index: value},\n",
        "            \"MDD\":  {index: value},\n",
        "            \"CS\":   {index: value},\n",
        "            \"LC\":   {index: value},\n",
        "            \"CoH\":  {index: value},\n",
        "        }\n",
        "    \"\"\"\n",
        "    mtld_res: Dict[Any, Optional[float]] = {}\n",
        "    ld_res: Dict[Any, Optional[float]] = {}\n",
        "    ls_res: Dict[Any, Optional[float]] = {}\n",
        "    mdd_res: Dict[Any, Optional[float]] = {}\n",
        "    cs_res: Dict[Any, Optional[float]] = {}\n",
        "\n",
        "    items = list(df[column].items())  # list[(index, text)]\n",
        "    total_items = len(items)\n",
        "\n",
        "    # ---- Lexical + syntactic (stanza) ----\n",
        "    for idx, text in tqdm(\n",
        "        items,\n",
        "        total=total_items,\n",
        "        desc=\"Computing lexical & syntactic complexity (sequential)\",\n",
        "    ):\n",
        "        metrics = _analyze_text_all(text, lang=lang)\n",
        "        mtld_res[idx] = metrics[\"MTLD\"]\n",
        "        ld_res[idx] = metrics[\"LD\"]\n",
        "        ls_res[idx] = metrics[\"LS\"]\n",
        "        mdd_res[idx] = metrics[\"MDD\"]\n",
        "        cs_res[idx] = metrics[\"CS\"]\n",
        "\n",
        "\n",
        "    # ---- Discourse measures (spaCy: LC & CoH) ----\n",
        "    if spacy_nlp is None:\n",
        "        raise ValueError(\n",
        "            \"spacy_nlp must be provided to compute LC and CoH. \"\n",
        "            \"Load a spaCy model with vectors, e.g. 'en_core_web_md', and \"\n",
        "            \"pass it as spacy_nlp=...\"\n",
        "        )\n",
        "\n",
        "    discourse = compute_discourse_measures(df, spacy_nlp, column=column)\n",
        "    lc_vec = discourse[\"LC\"]\n",
        "    coh_vec = discourse[\"CoH\"]\n",
        "\n",
        "    lc_res: Dict[Any, float] = {}\n",
        "    coh_res: Dict[Any, float] = {}\n",
        "\n",
        "    # Map arrays back to DataFrame indices\n",
        "    for i, idx in enumerate(df.index):\n",
        "        lc_res[idx] = float(lc_vec[i])\n",
        "        coh_res[idx] = float(coh_vec[i])\n",
        "\n",
        "    return {\n",
        "        \"MTLD\": mtld_res,\n",
        "        \"LD\": ld_res,\n",
        "        \"LS\": ls_res,\n",
        "        \"MDD\": mdd_res,\n",
        "        \"CS\": cs_res,\n",
        "        \"LC\": lc_res,\n",
        "        \"CoH\": coh_res,\n",
        "    }"
      ],
      "metadata": {
        "id": "Kktt6jbMclge"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Example script: load a DataFrame and compute all complexity measures.\n",
        "\"\"\"\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "\n",
        "    df_example = df.sample(n=5, random_state=12) # We sample 5 random rows\n",
        "    # Compute all measures for Simple texts\n",
        "    metrics = compute_all_complexity_measures_df(\n",
        "        df_example,\n",
        "        column=\"Simple\", # Note that we use the column \"Simple\" for the Simple text. Use 'Complex' for the Complex text.\n",
        "        lang=\"en\",\n",
        "\n",
        "        spacy_nlp=spacy_nlp\n",
        "    )\n",
        "\n",
        "    print(\"All complexity measures (per row):\")\n",
        "    pprint(metrics)"
      ],
      "metadata": {
        "id": "jExAN18Gjlz3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Drive mounting"
      ],
      "metadata": {
        "id": "vbBoGwZct_M9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "import os\n",
        "\n",
        "# Check if Google Drive is already mounted by checking for the presence of a common directory within the mount point\n",
        "if not os.path.exists('/content/drive/MyDrive'):\n",
        "    print(\"Mounting Google Drive...\")\n",
        "    drive.mount('/content/drive')\n",
        "else:\n",
        "    print(\"Google Drive is already mounted.\")\n",
        "\n",
        "# Crea una cartella dedicata per non fare confusione nel Drive\n",
        "save_path = \"/content/drive/MyDrive/Esame_Complexity\"\n",
        "if not os.path.exists(save_path):\n",
        "    try:\n",
        "        os.makedirs(save_path)\n",
        "        print(f\"Created directory: {save_path}\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error creating directory {save_path}: {e}\")"
      ],
      "metadata": {
        "id": "pvy_bvVb8W_F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Test"
      ],
      "metadata": {
        "id": "LGU9nqXZ89nC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "def run_sanity_check(n_rows=2):\n",
        "    print(\"=== AVVIO TEST DI FUNZIONAMENTO ===\")\n",
        "\n",
        "    # 1. Verifica Drive\n",
        "    if not os.path.exists(\"/content/drive/MyDrive\"):\n",
        "        print(\"❌ ERRORE: Google Drive non montato. Esegui la cella di mount.\")\n",
        "        return\n",
        "    else:\n",
        "        print(\"✅ Google Drive: Connesso\")\n",
        "\n",
        "    # 2. Verifica Dataset originali\n",
        "    first_ds_name = list(datasets.keys())[0]\n",
        "    first_ds_path = datasets[first_ds_name]\n",
        "    if not os.path.exists(first_ds_path):\n",
        "        print(f\"❌ ERRORE: Non trovo il file {first_ds_path}. Carica la cartella data_sampled.\")\n",
        "        return\n",
        "    else:\n",
        "        print(f\"✅ File sorgente trovati ({first_ds_name})\")\n",
        "\n",
        "    # 3. Test calcolo su piccola scala\n",
        "    print(f\"\\nTentativo di calcolo su {n_rows} righe...\")\n",
        "    test_df = load_dataset(first_ds_name).head(n_rows)\n",
        "\n",
        "    try:\n",
        "        # Testiamo una riga con Stanza\n",
        "        sample_text = test_df['Simple'].iloc[0]\n",
        "        print(f\"Test Stanza... \", end=\"\")\n",
        "        doc_s = get_stanza_pipeline('en')(sample_text)\n",
        "        _ = lexical_measures_from_doc(doc_s)\n",
        "        print(\"OK\")\n",
        "\n",
        "        # Testiamo una riga con spaCy\n",
        "        print(f\"Test spaCy... \", end=\"\")\n",
        "        _ = lexical_cohesion_single(sample_text, spacy_nlp)\n",
        "        print(\"OK\")\n",
        "\n",
        "        # 4. Test Scrittura su Drive\n",
        "        test_file = os.path.join(DRIVE_FOLDER, \"test_permessi.txt\")\n",
        "        with open(test_file, \"w\") as f:\n",
        "            f.write(\"Test scrittura riuscito\")\n",
        "        print(f\"✅ Test scrittura su Drive: OK\")\n",
        "        os.remove(test_file)\n",
        "\n",
        "        print(\"\\n=== TUTTO FUNZIONA CORRETTAMENTE ===\")\n",
        "        print(\"Puoi procedere con l'esecuzione della funzione principale.\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"\\n❌ ERRORE DURANTE IL TEST: {e}\")\n",
        "        print(\"Controlla le funzioni di calcolo o la connessione GPU.\")\n",
        "\n",
        "# Esecuzione del test\n",
        "run_sanity_check()"
      ],
      "metadata": {
        "id": "cNmVirzR8-8E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "# --- CONFIGURAZIONE DATASET ---\n",
        "datasets = {\n",
        "    #'ose_adv_ele': 'OSE_adv_ele.csv',\n",
        "    #'ose_adv_int': 'OSE_adv_int.csv',\n",
        "    #'swipe': 'swipe.csv',\n",
        "    'vikidia': 'vikidia.csv'\n",
        "}\n",
        "\n",
        "# Percorso su Google Drive definito nel Passaggio 1\n",
        "DRIVE_FOLDER = \"/content/drive/MyDrive/Esame_Complexity\"\n",
        "\n",
        "def load_data(path):\n",
        "    return pd.read_csv(path, sep='\\t')\n",
        "\n",
        "def load_dataset(name):\n",
        "    if name not in datasets:\n",
        "        raise ValueError(f\"Dataset {name} not found\")\n",
        "    return load_data(datasets[name])\n",
        "\n",
        "# --- BLOCCO DI COMPUTAZIONE AUTOMATIZZATO SU DRIVE ---\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    text_columns = ['Simple', 'Complex']\n",
        "    metrics_list = [\"MTLD\", \"LD\", \"LS\", \"MDD\", \"CS\", \"LC\", \"CoH\"]\n",
        "\n",
        "    for nome_ds in datasets.keys():\n",
        "        print(f\"\\n{'='*60}\")\n",
        "        print(f\"ELABORAZIONE DATASET: {nome_ds.upper()}\")\n",
        "\n",
        "        # Percorsi file puntati su DRIVE\n",
        "        checkpoint_path = os.path.join(DRIVE_FOLDER, f\"checkpoint_{nome_ds}.csv\")\n",
        "        final_output_path = os.path.join(DRIVE_FOLDER, f\"final_complexity_{nome_ds}.csv\")\n",
        "\n",
        "        # 1. Salto se già completato\n",
        "        if os.path.exists(final_output_path):\n",
        "            print(f\"Risultato finale già presente su Drive. Salto al prossimo dataset.\")\n",
        "            continue\n",
        "\n",
        "        # 2. Caricamento Dati o Ripristino Checkpoint da Drive\n",
        "        if os.path.exists(checkpoint_path):\n",
        "            print(f\"Ripristino progresso dal file di Drive...\")\n",
        "            working_df = pd.read_csv(checkpoint_path, sep='\\t')\n",
        "        else:\n",
        "            print(f\"Nessun checkpoint. Caricamento file originale...\")\n",
        "            working_df = load_dataset(nome_ds)\n",
        "            for t_col in text_columns:\n",
        "                for m in metrics_list:\n",
        "                    col_name = f\"{t_col}_{m}\"\n",
        "                    if col_name not in working_df.columns:\n",
        "                        working_df[col_name] = np.nan\n",
        "\n",
        "        # 3. Loop di calcolo\n",
        "        for t_col in text_columns:\n",
        "            current_metric_cols = [f\"{t_col}_{m}\" for m in metrics_list]\n",
        "\n",
        "            for idx, row in tqdm(working_df.iterrows(), total=len(working_df), desc=f\"{nome_ds} ({t_col})\"):\n",
        "\n",
        "                # Se la riga è già calcolata nel checkpoint, salta\n",
        "                if not working_df.loc[idx, current_metric_cols].isnull().any():\n",
        "                    continue\n",
        "\n",
        "                text = str(row[t_col])\n",
        "                if not text.strip() or text == 'nan':\n",
        "                    continue\n",
        "\n",
        "                try:\n",
        "                    # Analisi Stanza\n",
        "                    doc_stanza = get_stanza_pipeline('en')(text)\n",
        "\n",
        "                    lex = lexical_measures_from_doc(doc_stanza)\n",
        "                    working_df.at[idx, f\"{t_col}_MTLD\"] = lex.get(\"MTLD\")\n",
        "                    working_df.at[idx, f\"{t_col}_LD\"]   = lex.get(\"LD\")\n",
        "                    working_df.at[idx, f\"{t_col}_LS\"]   = lex.get(\"LS\")\n",
        "\n",
        "                    syn = syntactic_measures_from_doc(doc_stanza)\n",
        "                    working_df.at[idx, f\"{t_col}_MDD\"]  = syn.get(\"MDD\")\n",
        "                    working_df.at[idx, f\"{t_col}_CS\"]   = syn.get(\"CS\")\n",
        "\n",
        "                    # Analisi spaCy\n",
        "                    working_df.at[idx, f\"{t_col}_LC\"] = lexical_cohesion_single(text, spacy_nlp)\n",
        "                    working_df.at[idx, f\"{t_col}_CoH\"] = coherence_single(text, spacy_nlp)\n",
        "\n",
        "                except Exception as e:\n",
        "                    print(f\"\\n[ERRORE] Dataset {nome_ds}, Idx {idx}: {e}\")\n",
        "\n",
        "                # Salvataggio di sicurezza su DRIVE ogni 20 righe\n",
        "                if idx % 20 == 0:\n",
        "                    working_df.to_csv(checkpoint_path, sep='\\t', index=False)\n",
        "\n",
        "            # Salvataggio fine colonna\n",
        "            working_df.to_csv(checkpoint_path, sep='\\t', index=False)\n",
        "\n",
        "        # 4. Finalizzazione e pulizia NaN\n",
        "        print(f\"Generazione file finale pulito per {nome_ds}...\")\n",
        "        all_metric_cols = [f\"{tc}_{m}\" for tc in text_columns for m in metrics_list]\n",
        "\n",
        "        final_df = working_df.dropna(subset=all_metric_cols).copy()\n",
        "        final_df = final_df[text_columns + all_metric_cols]\n",
        "\n",
        "        # Salvataggio finale su DRIVE\n",
        "        final_df.to_csv(final_output_path, sep='\\t', index=False)\n",
        "        print(f\"✓ SUCCESSO: {final_output_path}\")\n",
        "\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"TUTTI I DATASET SONO STATI SALVATI SUL TUO DRIVE!\")\n",
        "    print(\"=\"*60)"
      ],
      "metadata": {
        "id": "W8-ecGlS8pqi"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}