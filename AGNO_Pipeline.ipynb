{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "59w3ONdDEmOI"
      },
      "source": [
        "Functions from data treatment have been copy-pasted here: while not the best practice. it allows for this notebook to be completely independent from other files."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Imports haven't been formatted yet. Expect a lot of repetitions"
      ],
      "metadata": {
        "id": "KdQXBhq33zN7"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Tp93MUYZFFvL"
      },
      "outputs": [],
      "source": [
        "# @title\n",
        "import sys\n",
        "!{sys.executable} -m pip install nltk\n",
        "!{sys.executable} -m pip install textcomplexity\n",
        "!{sys.executable} -m pip install stanza\n",
        "!{sys.executable} -m pip install wordfreq\n",
        "!{sys.executable} -m spacy download en_core_web_md\n",
        "!{sys.executable} -m pip install tqdm spacy numpy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8O66Awg7FTT0"
      },
      "outputs": [],
      "source": [
        "# @title\n",
        "# Standard library imports\n",
        "import json\n",
        "from collections import Counter\n",
        "from functools import lru_cache\n",
        "from pprint import pprint\n",
        "from typing import Dict, Set, Iterable, Optional, Any, Tuple\n",
        "import importlib.resources as pkg_resources\n",
        "\n",
        "# Third-party imports\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import nltk\n",
        "from nltk.corpus import wordnet as wn\n",
        "import spacy\n",
        "import stanza\n",
        "import textcomplexity  # only used to access en.json\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "# Download required resources\n",
        "stanza.download('en')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n",
        "\n",
        "# Make sure WordNet is available; if not, download it.\n",
        "try:\n",
        "    _ = wn.synsets(\"dog\")\n",
        "except LookupError:\n",
        "    nltk.download(\"wordnet\")\n",
        "    nltk.download(\"omw-1.4\")\n",
        "\n",
        "# Load spaCy model\n",
        "nlp = spacy.load(\"en_core_web_md\", disable=[\"ner\", \"textcat\"])\n",
        "spacy_nlp = nlp\n",
        "spacy_nlp.add_pipe(\"sentencizer\")\n",
        "\n",
        "\n",
        "# Stanza pipeline cache\n",
        "@lru_cache(maxsize=None)  # Cache pipelines for different languages\n",
        "def get_stanza_pipeline(lang: str):\n",
        "    if lang == 'en':\n",
        "        # Use pre-trained models for English, including constituency parser\n",
        "        # for CS metric and dependency parser for MDD metric\n",
        "        return stanza.Pipeline(lang='en', processors='tokenize,pos,lemma,depparse,constituency')\n",
        "    else:\n",
        "        raise ValueError(f\"Unsupported language: {lang}\")\n",
        "\n",
        "# Define CONTENT_UPOS - this was missing previously, causing an error if not defined globally or locally where used\n",
        "# Based on the usage in _compute_lexical_density, these are Universal POS tags for content words\n",
        "CONTENT_UPOS = {\"NOUN\", \"PROPN\", \"ADJ\", \"VERB\", \"ADV\"}\n",
        "\n",
        "# Define CONTENT_POS - this was also missing previously and is used in the discourse complexity functions\n",
        "CONTENT_POS =  {\"NOUN\", \"VERB\", \"ADJ\", \"ADV\"}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lLjmDfIkjUuf"
      },
      "source": [
        "Debug"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "BYuEUkFOFf9_"
      },
      "outputs": [],
      "source": [
        "# @title\n",
        "# Cache stanza pipelines to avoid re-loading models\n",
        "_STANZA_PIPELINES: Dict[str, stanza.Pipeline] = {}\n",
        "\n",
        "# UPOS tags considered content words (C)\n",
        "CONTENT_UPOS = {\"NOUN\", \"PROPN\", \"VERB\", \"ADJ\", \"ADV\"}\n",
        "\n",
        "\n",
        "@lru_cache()\n",
        "def load_cow_top5000_en() -> Set[str]:\n",
        "    \"\"\"\n",
        "    Load the COW-based list of the 5,000 most frequent English content words\n",
        "    from textcomplexity's English language definition file (en.json).\n",
        "\n",
        "    We ignore POS tags and keep only lowercased word forms.\n",
        "    \"\"\"\n",
        "    with pkg_resources.files(textcomplexity).joinpath(\"en.json\").open(\n",
        "        \"r\", encoding=\"utf-8\"\n",
        "    ) as f:\n",
        "        lang_def = json.load(f)\n",
        "\n",
        "    most_common = lang_def[\"most_common\"]  # list of [word, xpos]\n",
        "    cow_top5000 = {w.lower() for w, xpos in most_common}\n",
        "    return cow_top5000\n",
        "\n",
        "\n",
        "def get_stanza_pipeline(lang: str = \"en\", use_gpu: bool = True) -> stanza.Pipeline:\n",
        "    \"\"\"\n",
        "    Get (or create) a cached stanza Pipeline for a given language.\n",
        "\n",
        "    NOTE: You must have downloaded the models beforehand, e.g.:\n",
        "        import stanza\n",
        "        stanza.download('en')\n",
        "    \"\"\"\n",
        "    if lang not in _STANZA_PIPELINES:\n",
        "        _STANZA_PIPELINES[lang] = stanza.Pipeline(\n",
        "            lang=lang,\n",
        "            processors=\"tokenize,pos,lemma,depparse,constituency\",\n",
        "            use_gpu=use_gpu,\n",
        "            tokenize_no_ssplit=False,\n",
        "        )\n",
        "    return _STANZA_PIPELINES[lang]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "bysT9g26FoXc"
      },
      "outputs": [],
      "source": [
        "# @title\n",
        "def _compute_mtld(tokens: Iterable[str], ttr_threshold: float = 0.72) -> Optional[float]:\n",
        "    \"\"\"\n",
        "    Compute MTLD (Measure of Textual Lexical Diversity) for a list of tokens.\n",
        "\n",
        "    MTLD = total_number_of_tokens / number_of_factors\n",
        "\n",
        "    A factor is a contiguous segment where the running TTR stays >= threshold.\n",
        "    When the TTR drops below the threshold, we close a factor (at the previous\n",
        "    token) and start a new one. At the end, the remaining partial segment is\n",
        "    counted as a fractional factor, with weight proportional to how close the\n",
        "    final TTR is to the threshold.\n",
        "    \"\"\"\n",
        "    tokens = [tok for tok in tokens if tok]\n",
        "    if not tokens:\n",
        "        return None\n",
        "\n",
        "    types = set()\n",
        "    factor_count = 0.0\n",
        "    token_count_in_factor = 0\n",
        "\n",
        "    for tok in tokens:\n",
        "        token_count_in_factor += 1\n",
        "        types.add(tok)\n",
        "        ttr = len(types) / token_count_in_factor\n",
        "\n",
        "        if ttr < ttr_threshold:\n",
        "            factor_count += 1.0\n",
        "            types = set()\n",
        "            token_count_in_factor = 0\n",
        "\n",
        "    # final partial factor\n",
        "    if token_count_in_factor > 0:\n",
        "        final_ttr = len(types) / token_count_in_factor\n",
        "        if final_ttr < 1.0:\n",
        "            fractional = (1.0 - final_ttr) / (1.0 - ttr_threshold)\n",
        "            fractional = max(0.0, min(1.0, fractional))\n",
        "            factor_count += fractional\n",
        "\n",
        "    if factor_count == 0:\n",
        "        return None\n",
        "\n",
        "    return len(tokens) / factor_count\n",
        "\n",
        "\n",
        "\n",
        "def _compute_lexical_density(total_tokens: int, content_tokens: int) -> Optional[float]:\n",
        "    \"\"\"\n",
        "    LD = |C| / |T|\n",
        "    where:\n",
        "        |C| = number of content-word tokens\n",
        "        |T| = total number of non-punctuation tokens\n",
        "    \"\"\"\n",
        "    if total_tokens == 0:\n",
        "        return None\n",
        "    return content_tokens / total_tokens\n",
        "\n",
        "\n",
        "def _compute_lexical_sophistication_cow(\n",
        "    content_forms: Iterable[str],\n",
        "    cow_top5000: set,\n",
        ") -> Optional[float]:\n",
        "    \"\"\"\n",
        "    LS = |{ w in C : w not in R }| / |C|\n",
        "    where:\n",
        "        C = content-word tokens (surface forms, lowercased)\n",
        "        R = COW top-5000 content word forms (lowercased)\n",
        "    \"\"\"\n",
        "    forms = [f for f in content_forms if f]\n",
        "    if not forms:\n",
        "        return None\n",
        "\n",
        "    off_list = sum(1 for f in forms if f not in cow_top5000)\n",
        "    return off_list / len(forms)\n",
        "\n",
        "\n",
        "@lru_cache(maxsize=1)\n",
        "def load_cow_top5000_en() -> Set[str]:\n",
        "    \"\"\"\n",
        "    Load the COW top-5000 English content word forms from textcomplexity package data.\n",
        "    The list is expected to be in 'textcomplexity/data/en.json' under the key 'cow_top5000'.\n",
        "    \"\"\"\n",
        "    # Use importlib.resources.files for modern package data access\n",
        "    json_path = pkg_resources.files('textcomplexity').joinpath('en.json')\n",
        "    with json_path.open('r') as f:\n",
        "        data = json.load(f)\n",
        "        return set(data.get(\"cow_top5000\", []))\n",
        "\n",
        "\n",
        "def lexical_measures_from_doc(doc) -> Dict[str, Optional[float]]:\n",
        "    \"\"\"\n",
        "    Compute MTLD, LD, LS from a stanza Document.\n",
        "    \"\"\"\n",
        "    cow_top5000 = load_cow_top5000_en()\n",
        "\n",
        "    mtld_tokens = []\n",
        "    total_tokens = 0\n",
        "    content_tokens = 0\n",
        "    content_forms = []\n",
        "\n",
        "    for sent in doc.sentences:\n",
        "        for word in sent.words:\n",
        "            if word.upos == \"PUNCT\":\n",
        "                continue\n",
        "\n",
        "            lemma = (word.lemma or word.text or \"\").lower()\n",
        "            if not lemma:\n",
        "                continue\n",
        "\n",
        "            mtld_tokens.append(lemma)\n",
        "            total_tokens += 1\n",
        "\n",
        "            if word.upos in CONTENT_UPOS:\n",
        "                content_tokens += 1\n",
        "                form = (word.text or \"\").lower()\n",
        "                content_forms.append(form)\n",
        "\n",
        "    mtld = _compute_mtld(mtld_tokens) if mtld_tokens else None\n",
        "    ld = _compute_lexical_density(total_tokens, content_tokens)\n",
        "    ls = _compute_lexical_sophistication_cow(content_forms, cow_top5000)\n",
        "\n",
        "    return {\"MTLD\": mtld, \"LD\": ld, \"LS\": ls}\n",
        "\n",
        "\n",
        "def lexical_measures_from_text(text: str, lang: str = \"en\") -> Dict[str, Optional[float]]:\n",
        "    \"\"\"\n",
        "    Convenience wrapper: parse a single text and compute lexical measures.\n",
        "    \"\"\"\n",
        "    if text is None:\n",
        "        text = \"\"\n",
        "    text = str(text)\n",
        "\n",
        "    if not text.strip():\n",
        "        return {\"MTLD\": None, \"LD\": None, \"LS\": None}\n",
        "\n",
        "    nlp = get_stanza_pipeline(lang)\n",
        "    doc = nlp(text)\n",
        "    return lexical_measures_from_doc(doc)\n",
        "\n",
        "\n",
        "\n",
        "def compute_lexical_measures_df(\n",
        "    df: pd.DataFrame,\n",
        "    column: str = \"text\",\n",
        "    lang: str = \"en\",\n",
        ") -> Dict[str, Dict[Any, Optional[float]]]:\n",
        "    \"\"\"\n",
        "    Compute lexical measures for each row in df[column].\n",
        "\n",
        "    Returns:\n",
        "        {\n",
        "            \"MTLD\": {index: value},\n",
        "            \"LD\":   {index: value},\n",
        "            \"LS\":   {index: value},\n",
        "        }\n",
        "    \"\"\"\n",
        "    mtld_res: Dict[Any, Optional[float]] = {}\n",
        "    ld_res: Dict[Any, Optional[float]] = {}\n",
        "    ls_res: Dict[Any, Optional[float]] = {}\n",
        "\n",
        "    for idx, text in df[column].items():\n",
        "        metrics = lexical_measures_from_text(text, lang=lang)\n",
        "        mtld_res[idx] = metrics[\"MTLD\"]\n",
        "        ld_res[idx] = metrics[\"LD\"]\n",
        "        ls_res[idx] = metrics[\"LS\"]\n",
        "\n",
        "    return {\"MTLD\": mtld_res, \"LD\": ld_res, \"LS\": ls_res}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "n-htFIqzFpCH"
      },
      "outputs": [],
      "source": [
        "# @title\n",
        "def mdd_from_doc(doc) -> Optional[float]:\n",
        "    \"\"\"\n",
        "    Compute Mean Dependency Distance (MDD) from a stanza Document.\n",
        "\n",
        "    For each sentence s_i with dependency set D_i:\n",
        "        MDD_i = (1 / |D_i|) * sum_{(h,d) in D_i} |h - d|\n",
        "    Then:\n",
        "        MDD = (1 / k) * sum_i MDD_i, over all sentences with at least one dependency.\n",
        "    \"\"\"\n",
        "    sentence_mdds = []\n",
        "\n",
        "    for sent in doc.sentences:\n",
        "        distances = []\n",
        "        for w in sent.words:\n",
        "            if w.head is None or w.head == 0:\n",
        "                continue\n",
        "            distances.append(abs(w.id - w.head))\n",
        "\n",
        "        if distances:\n",
        "            sentence_mdds.append(sum(distances) / len(distances))\n",
        "\n",
        "    if not sentence_mdds:\n",
        "        return None\n",
        "    return sum(sentence_mdds) / len(sentence_mdds)\n",
        "\n",
        "\n",
        "\n",
        "def _count_clauses_in_tree(tree) -> int:\n",
        "    \"\"\"\n",
        "    Count clause nodes in a constituency tree.\n",
        "\n",
        "    A simple and standard heuristic (PTB-style) is:\n",
        "        count all nodes whose label starts with 'S'\n",
        "        (S, SBAR, SBARQ, SINV, SQ, etc.).\n",
        "\n",
        "    This aligns with the idea of counting finite and subordinate clauses\n",
        "    as in Hunt (1965) and later complexity work.\n",
        "    \"\"\"\n",
        "    if tree is None:\n",
        "        return 0\n",
        "\n",
        "    # Stanza's constituency tree: tree.label, tree.children\n",
        "    count = 1 if getattr(tree, \"label\", \"\").startswith(\"S\") else 0\n",
        "\n",
        "    for child in getattr(tree, \"children\", []):\n",
        "        # leaves can be strings or terminals without 'label'\n",
        "        if hasattr(child, \"label\"):\n",
        "            count += _count_clauses_in_tree(child)\n",
        "\n",
        "    return count\n",
        "\n",
        "\n",
        "def cs_from_doc(doc) -> Optional[float]:\n",
        "    \"\"\"\n",
        "    Compute CS (clauses per sentence) from a stanza Document.\n",
        "\n",
        "        CS = (1 / k) * sum_i L_i\n",
        "\n",
        "    where L_i is the number of clauses in sentence s_i, estimated by counting\n",
        "    all constituents whose label starts with 'S' in the constituency tree of s_i.\n",
        "    \"\"\"\n",
        "    clause_counts = []\n",
        "    for sent in doc.sentences:\n",
        "        tree = getattr(sent, \"constituency\", None)\n",
        "        if tree is None:\n",
        "            # No constituency tree available for this sentence\n",
        "            continue\n",
        "        num_clauses = _count_clauses_in_tree(tree)\n",
        "        clause_counts.append(num_clauses)\n",
        "\n",
        "    if not clause_counts:\n",
        "        return None\n",
        "\n",
        "    return sum(clause_counts) / len(clause_counts)\n",
        "\n",
        "\n",
        "\n",
        "def syntactic_measures_from_doc(doc) -> Dict[str, Optional[float]]:\n",
        "    \"\"\"\n",
        "    Compute MDD and CS from a stanza Document.\n",
        "    \"\"\"\n",
        "    mdd = mdd_from_doc(doc)\n",
        "    cs = cs_from_doc(doc)\n",
        "    return {\"MDD\": mdd, \"CS\": cs}\n",
        "\n",
        "\n",
        "def syntactic_measures_from_text(text: str, lang: str = \"en\") -> Dict[str, Optional[float]]:\n",
        "    \"\"\"\n",
        "    Convenience wrapper: parse a single text and compute syntactic measures.\n",
        "    \"\"\"\n",
        "    if text is None:\n",
        "        text = \"\"\n",
        "    text = str(text)\n",
        "\n",
        "    if not text.strip():\n",
        "        return {\"MDD\": None, \"CS\": None}\n",
        "\n",
        "    nlp = get_stanza_pipeline(lang)\n",
        "    doc = nlp(text)\n",
        "    return syntactic_measures_from_doc(doc)\n",
        "\n",
        "\n",
        "def compute_syntactic_measures_df(\n",
        "    df: pd.DataFrame,\n",
        "    column: str = \"text\",\n",
        "    lang: str = \"en\",\n",
        ") -> Dict[str, Dict[Any, Optional[float]]]:\n",
        "    \"\"\"\n",
        "    Compute syntactic measures for each row in df[column].\n",
        "\n",
        "    Returns:\n",
        "        {\n",
        "            \"MDD\": {index: value},\n",
        "            \"CS\":  {index: value},\n",
        "        }\n",
        "    \"\"\"\n",
        "    mdd_res: Dict[Any, Optional[float]] = {}\n",
        "    cs_res: Dict[Any, Optional[float]] = {}\n",
        "\n",
        "    for idx, text in df[column].items():\n",
        "        metrics = syntactic_measures_from_text(text, lang=lang)\n",
        "        mdd_res[idx] = metrics[\"MDD\"]\n",
        "        cs_res[idx] = metrics[\"CS\"]\n",
        "\n",
        "    return {\"MDD\": mdd_res, \"CS\": cs_res}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "bpCgoyhYFvgf"
      },
      "outputs": [],
      "source": [
        "# @title\n",
        "# Approximate set of content POS tags (spaCy universal POS)\n",
        "CONTENT_POS =  {\"NOUN\", \"VERB\", \"ADJ\", \"ADV\"}\n",
        "\n",
        "\n",
        "def is_content_token(tok):\n",
        "    \"\"\"\n",
        "    Return True if token is considered a content word.\n",
        "    We ignore stopwords, punctuation, and non-alphabetic tokens.\n",
        "    \"\"\"\n",
        "    return (\n",
        "        tok.is_alpha\n",
        "        and not tok.is_stop\n",
        "        and tok.pos_ in CONTENT_POS\n",
        "    )\n",
        "\n",
        "\n",
        "@lru_cache(maxsize=100000)\n",
        "def get_related_lemmas(lemma):\n",
        "    \"\"\"\n",
        "    Return a set of semantically related lemmas for the given lemma\n",
        "    using WordNet, including:\n",
        "      - synonyms\n",
        "      - antonyms\n",
        "      - hypernyms / hyponyms\n",
        "      - meronyms (part/member/substance)\n",
        "      - coordinate terms (siblings under the same hypernym)\n",
        "\n",
        "    NOTE: Some older examples mention 'troponyms', but in NLTK's\n",
        "    WordNet interface there is no 'troponyms()' method on Synset,\n",
        "    so we do NOT use it here.\n",
        "    \"\"\"\n",
        "    lemma = lemma.lower()\n",
        "    related = set()\n",
        "    synsets = wn.synsets(lemma)\n",
        "\n",
        "    for syn in synsets:\n",
        "        # Synonyms and antonyms\n",
        "        for l in syn.lemmas():\n",
        "            related.add(l.name().lower().replace(\"_\", \" \"))\n",
        "            for ant in l.antonyms():\n",
        "                related.add(ant.name().lower().replace(\"_\", \" \"))\n",
        "\n",
        "        # Hypernyms (more general) and hyponyms (more specific)\n",
        "        for hyper in syn.hypernyms():\n",
        "            for l in hyper.lemmas():\n",
        "                related.add(l.name().lower().replace(\"_\", \" \"))\n",
        "        for hypo in syn.hyponyms():\n",
        "            for l in hypo.lemmas():\n",
        "                related.add(l.name().lower().replace(\"_\", \" \"))\n",
        "\n",
        "        # Meronyms: part/member/substance\n",
        "        for mer in syn.part_meronyms() + syn.member_meronyms() + syn.substance_meronyms():\n",
        "            for l in mer.lemmas():\n",
        "                related.add(l.name().lower().replace(\"_\", \" \"))\n",
        "\n",
        "        # Coordinate terms (siblings under same hypernym)\n",
        "        for hyper in syn.hypernyms():\n",
        "            for sibling in hyper.hyponyms():\n",
        "                if sibling == syn:\n",
        "                    continue\n",
        "                for l in sibling.lemmas():\n",
        "                    related.add(l.name().lower().replace(\"_\", \" \"))\n",
        "\n",
        "    # Remove the lemma itself if present\n",
        "    related.discard(lemma)\n",
        "    return related\n",
        "\n",
        "\n",
        "def lexical_cohesion_single(text, nlp):\n",
        "    \"\"\"\n",
        "    Compute Lexical Cohesion (LC) for a single document:\n",
        "\n",
        "        LC = |C| / m\n",
        "\n",
        "    where:\n",
        "      - |C| is the number of cohesive devices between sentences\n",
        "        (lexical repetition + semantic relations),\n",
        "      - m  is the total number of word tokens (alphabetic) in the document.\n",
        "\n",
        "    If the document has fewer than 2 sentences or no valid words,\n",
        "    LC is returned as 0.0.\n",
        "    \"\"\"\n",
        "    if not isinstance(text, str) or not text.strip():\n",
        "        return 0.0\n",
        "\n",
        "    doc = nlp(text)\n",
        "\n",
        "    # Total number of alphabetic tokens (denominator m)\n",
        "    m = sum(1 for tok in doc if tok.is_alpha)\n",
        "    if m == 0:\n",
        "        return 0.0\n",
        "\n",
        "    sentences = list(doc.sents)\n",
        "    if len(sentences) < 2:\n",
        "        # With only one sentence, cross-sentence cohesion is not defined\n",
        "        return 0.0\n",
        "\n",
        "    # Collect sets of content lemmas per sentence\n",
        "    sent_lemmas = []\n",
        "    for sent in sentences:\n",
        "        lemmas = set(\n",
        "            tok.lemma_.lower()\n",
        "            for tok in sent\n",
        "            if is_content_token(tok)\n",
        "        )\n",
        "        if lemmas:\n",
        "            sent_lemmas.append(lemmas)\n",
        "\n",
        "    if len(sent_lemmas) < 2:\n",
        "        return 0.0\n",
        "\n",
        "    cohesive_count = 0\n",
        "\n",
        "    for i in range(len(sent_lemmas) - 1):\n",
        "        for j in range(i + 1, len(sent_lemmas)):\n",
        "            li = sent_lemmas[i]\n",
        "            lj = sent_lemmas[j]\n",
        "\n",
        "            # 1) Lexical repetition: shared lemmas\n",
        "            shared = li & lj\n",
        "            cohesive_count += len(shared)\n",
        "\n",
        "            # 2) Semantic relations via WordNet\n",
        "            for lemma in li:\n",
        "                related = get_related_lemmas(lemma)\n",
        "                cohesive_count += len(related & lj)\n",
        "\n",
        "    return float(cohesive_count) / float(m)\n",
        "\n",
        "\n",
        "def sentence_vector(sent, vector_size):\n",
        "    \"\"\"\n",
        "    Represent a sentence as the average of token vectors.\n",
        "    If no token has a vector, return a zero vector.\n",
        "    \"\"\"\n",
        "    vecs = [\n",
        "        tok.vector\n",
        "        for tok in sent\n",
        "        if tok.has_vector and not tok.is_punct and not tok.is_space\n",
        "    ]\n",
        "    if not vecs:\n",
        "        return np.zeros(vector_size, dtype=\"float32\")\n",
        "    return np.mean(vecs, axis=0)\n",
        "\n",
        "\n",
        "def coherence_single(text, nlp):\n",
        "    \"\"\"\n",
        "    Compute Coherence (CoH) for a single document as the average\n",
        "    cosine similarity between adjacent sentence vectors:\n",
        "\n",
        "        CoH = (1 / (k-1)) * sum_{i=1}^{k-1} cos(h_i, h_{i+1})\n",
        "\n",
        "    where h_i is the sentence/topic vector for sentence i.\n",
        "\n",
        "    If the document has fewer than 2 sentences, CoH = 0.0.\n",
        "    \"\"\"\n",
        "    if not isinstance(text, str) or not text.strip():\n",
        "        return 0.0\n",
        "\n",
        "    if nlp.vocab.vectors_length == 0:\n",
        "        raise ValueError(\n",
        "            \"The loaded spaCy model does not contain word vectors \"\n",
        "            \"(nlp.vocab.vectors_length == 0). \"\n",
        "            \"Use a model like 'en_core_web_md' or similar.\"\n",
        "        )\n",
        "\n",
        "    doc = nlp(text)\n",
        "    sentences = list(doc.sents)\n",
        "    k = len(sentences)\n",
        "\n",
        "    if k < 2:\n",
        "        # Only one sentence: no adjacent pair, coherence = 0.0\n",
        "        return 0.0\n",
        "\n",
        "    vector_size = nlp.vocab.vectors_length\n",
        "    sent_vectors = [\n",
        "        sentence_vector(sent, vector_size)\n",
        "        for sent in sentences\n",
        "    ]\n",
        "\n",
        "    sims = []\n",
        "    for i in range(k - 1):\n",
        "        v1 = sent_vectors[i]\n",
        "        v2 = sent_vectors[i + 1]\n",
        "        norm1 = np.linalg.norm(v1)\n",
        "        norm2 = np.linalg.norm(v2)\n",
        "        denom = norm1 * norm2\n",
        "        if denom == 0.0:\n",
        "            # Skip pairs where at least one sentence vector is zero\n",
        "            continue\n",
        "        cos_sim = float(np.dot(v1, v2) / denom)\n",
        "        sims.append(cos_sim)\n",
        "\n",
        "    if not sims:\n",
        "        return 0.0\n",
        "\n",
        "    return float(np.mean(sims))\n",
        "\n",
        "\n",
        "\n",
        "def compute_lexical_cohesion_vector(df, nlp, column=\"text\"):\n",
        "    \"\"\"\n",
        "    Compute LC for each row of a DataFrame.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    df : pandas.DataFrame\n",
        "        DataFrame containing the texts.\n",
        "    nlp : spaCy Language object\n",
        "        Pre-loaded spaCy pipeline with lemmatizer, POS tagger, etc.\n",
        "    column : str, default \"text\"\n",
        "        Name of the column that contains the text.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    np.ndarray\n",
        "        1D array of LC scores, length == len(df).\n",
        "    \"\"\"\n",
        "    texts = df[column].fillna(\"\").astype(str)\n",
        "    scores = [lexical_cohesion_single(t, nlp) for t in texts]\n",
        "    return np.array(scores, dtype=\"float32\")\n",
        "\n",
        "\n",
        "def compute_coherence_vector(df, nlp, column=\"text\"):\n",
        "    \"\"\"\n",
        "    Compute CoH for each row of a DataFrame.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    df : pandas.DataFrame\n",
        "        DataFrame containing the texts.\n",
        "    nlp : spaCy Language object\n",
        "        Pre-loaded spaCy pipeline with word vectors.\n",
        "    column : str, default \"text\"\n",
        "        Name of the column that contains the text.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    np.ndarray\n",
        "        1D array of CoH scores, length == len(df).\n",
        "    \"\"\"\n",
        "    texts = df[column].fillna(\"\").astype(str)\n",
        "    scores = [coherence_single(t, nlp) for t in texts]\n",
        "    return np.array(scores, dtype=\"float32\")\n",
        "\n",
        "\n",
        "def compute_discourse_measures(df, nlp, column=\"text\"):\n",
        "    \"\"\"\n",
        "    Compute both LC and CoH for each row of a DataFrame and return\n",
        "    them in a dictionary.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    dict\n",
        "        {\n",
        "            \"LC\":  np.ndarray of lexical cohesion scores,\n",
        "            \"CoH\": np.ndarray of coherence scores\n",
        "        }\n",
        "    \"\"\"\n",
        "    lc_vec = compute_lexical_cohesion_vector(df, nlp, column=column)\n",
        "    coh_vec = compute_coherence_vector(df, nlp, column=column)\n",
        "    return {\"LC\": lc_vec, \"CoH\": coh_vec}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "kot-cYkaFxZz"
      },
      "outputs": [],
      "source": [
        "# @title\n",
        "def _analyze_text_all(text: str, lang: str = \"en\") -> Dict[str, Optional[float]]:\n",
        "    \"\"\"\n",
        "    Parse a text with stanza and compute all measures (lexical + syntactic)\n",
        "    in a single pass.\n",
        "\n",
        "    Returns a dict with keys:\n",
        "        \"MTLD\", \"LD\", \"LS\", \"MDD\", \"CS\"\n",
        "    (Discourse measures LC/CoH are added later at DataFrame level, via spaCy.)\n",
        "    \"\"\"\n",
        "    if text is None:\n",
        "        text = \"\"\n",
        "    text = str(text)\n",
        "\n",
        "    if not text.strip():\n",
        "        return {\"MTLD\": None, \"LD\": None, \"LS\": None, \"MDD\": None, \"CS\": None}\n",
        "\n",
        "    nlp = get_stanza_pipeline(lang)\n",
        "    doc = nlp(text)\n",
        "\n",
        "    lex = lexical_measures_from_doc(doc)\n",
        "    syn = syntactic_measures_from_doc(doc)\n",
        "\n",
        "    out: Dict[str, Optional[float]] = {}\n",
        "    out.update(lex)\n",
        "    out.update(syn)\n",
        "    return out\n",
        "\n",
        "\n",
        "def compute_all_complexity_measures_df(\n",
        "    df: pd.DataFrame,\n",
        "    column: str = \"text\",\n",
        "    lang: str = \"en\",\n",
        "    spacy_nlp=None,\n",
        ") -> Dict[str, Dict[Any, Optional[float]]]:\n",
        "    \"\"\"\n",
        "    Compute all complexity measures for each row in df[column].\n",
        "\n",
        "    Args\n",
        "    ----\n",
        "    df : pandas.DataFrame\n",
        "        DataFrame with a text column.\n",
        "    column : str, default \"text\"\n",
        "        Name of the text column.\n",
        "    lang : str, default \"en\"\n",
        "        Language code for stanza.\n",
        "    n_jobs : int, default 1\n",
        "        Number of worker processes to use.\n",
        "            - 1  : sequential execution (no multiprocessing).\n",
        "            - >1 : multiprocessing with that many workers.\n",
        "            - 0 or None : use cpu_count() workers.\n",
        "    spacy_nlp : spaCy Language, required for LC / CoH\n",
        "        Pre-loaded spaCy pipeline with:\n",
        "            - POS / lemmatizer for LC\n",
        "            - word vectors for CoH (e.g. 'en_core_web_md').\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    dict\n",
        "        {\n",
        "            \"MTLD\": {index: value},\n",
        "            \"LD\":   {index: value},\n",
        "            \"LS\":   {index: value},\n",
        "            \"MDD\":  {index: value},\n",
        "            \"CS\":   {index: value},\n",
        "            \"LC\":   {index: value},\n",
        "            \"CoH\":  {index: value},\n",
        "        }\n",
        "    \"\"\"\n",
        "    mtld_res: Dict[Any, Optional[float]] = {}\n",
        "    ld_res: Dict[Any, Optional[float]] = {}\n",
        "    ls_res: Dict[Any, Optional[float]] = {}\n",
        "    mdd_res: Dict[Any, Optional[float]] = {}\n",
        "    cs_res: Dict[Any, Optional[float]] = {}\n",
        "\n",
        "    items = list(df[column].items())  # list[(index, text)]\n",
        "    total_items = len(items)\n",
        "\n",
        "    # ---- Lexical + syntactic (stanza) ----\n",
        "    for idx, text in tqdm(\n",
        "        items,\n",
        "        total=total_items,\n",
        "        desc=\"Computing lexical & syntactic complexity (sequential)\",\n",
        "    ):\n",
        "        metrics = _analyze_text_all(text, lang=lang)\n",
        "        mtld_res[idx] = metrics[\"MTLD\"]\n",
        "        ld_res[idx] = metrics[\"LD\"]\n",
        "        ls_res[idx] = metrics[\"LS\"]\n",
        "        mdd_res[idx] = metrics[\"MDD\"]\n",
        "        cs_res[idx] = metrics[\"CS\"]\n",
        "\n",
        "\n",
        "    # ---- Discourse measures (spaCy: LC & CoH) ----\n",
        "    if spacy_nlp is None:\n",
        "        raise ValueError(\n",
        "            \"spacy_nlp must be provided to compute LC and CoH. \"\n",
        "            \"Load a spaCy model with vectors, e.g. 'en_core_web_md', and \"\n",
        "            \"pass it as spacy_nlp=...\"\n",
        "        )\n",
        "\n",
        "    discourse = compute_discourse_measures(df, spacy_nlp, column=column)\n",
        "    lc_vec = discourse[\"LC\"]\n",
        "    coh_vec = discourse[\"CoH\"]\n",
        "\n",
        "    lc_res: Dict[Any, float] = {}\n",
        "    coh_res: Dict[Any, float] = {}\n",
        "\n",
        "    # Map arrays back to DataFrame indices\n",
        "    for i, idx in enumerate(df.index):\n",
        "        lc_res[idx] = float(lc_vec[i])\n",
        "        coh_res[idx] = float(coh_vec[i])\n",
        "\n",
        "    return {\n",
        "        \"MTLD\": mtld_res,\n",
        "        \"LD\": ld_res,\n",
        "        \"LS\": ls_res,\n",
        "        \"MDD\": mdd_res,\n",
        "        \"CS\": cs_res,\n",
        "        \"LC\": lc_res,\n",
        "        \"CoH\": coh_res,\n",
        "    }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "SV_cTNI0jVz8"
      },
      "outputs": [],
      "source": [
        "# @title\n",
        "stanza_nlp = get_stanza_pipeline(\"en\", use_gpu=True)\n",
        "\n",
        "# Crea i \"alias\" (puntatori) per le funzioni che cercano nomi diversi\n",
        "nlp_stanza = stanza_nlp\n",
        "nlp_spacy = spacy_nlp # Cella necessaria per un mio momento di confusione. Non sono nemmeno sicuro che serva più"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DBOng4DT79AJ"
      },
      "source": [
        "Preliminary Requirements.\n",
        "\n",
        "Installs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2nIFCigxG4hA"
      },
      "outputs": [],
      "source": [
        "!pip install -q transformers accelerate bitsandbytes agno jsonlines\n",
        "!pip install -U bitsandbytes\n",
        "!pip install -q agno transformers accelerate bitsandbytes sentencepiece protobuf"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IE9KAiEZ_VNT"
      },
      "source": [
        "Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F244aA2f_jFc"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import json\n",
        "import jsonlines\n",
        "import pandas as pd\n",
        "from datetime import datetime\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
        "from agno.agent import Agent\n",
        "\n",
        "METRICS_ORDER = ['MTLD', 'LD', 'LS', 'MDD', 'CS', 'LC', 'CoH']\n",
        "\n",
        "pip install ollama # nonostante venga installato dopo, per qualche motivo una singola installazione è assente senza questa riga."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7mn_SPrrArVO"
      },
      "source": [
        "LLM setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OZYH4vzGAuHJ"
      },
      "outputs": [],
      "source": [
        "# Installa Ollama\n",
        "!curl -fsSL https://ollama.com/install.sh | sh\n",
        "\n",
        "import subprocess\n",
        "import time\n",
        "import os\n",
        "\n",
        "# Avvia il server Ollama in background\n",
        "# Usiamo env per forzare il rilevamento della GPU se necessario\n",
        "env = os.environ.copy()\n",
        "env[\"OLLAMA_HOST\"] = \"127.0.0.1:11434\"\n",
        "\n",
        "print(\" Avvio server Ollama...\")\n",
        "with open(\"ollama.log\", \"w\") as f:\n",
        "    subprocess.Popen([\"ollama\", \"serve\"], stdout=f, stderr=f, env=env)\n",
        "\n",
        "# Attesa critica per l'inizializzazione del server\n",
        "time.sleep(10)\n",
        "\n",
        "# Scarica il modello richiesto\n",
        "#limitarsi a 1-2 modelli per volta per non riempire la VRAM, 4 modelli solo se stiamo usando una A100 o simili\n",
        "print(\" Download modello...\")\n",
        "!ollama pull qwen2.5:7b-instruct-q4_K_M\n",
        "!ollama pull llama3.1:8b\n",
        "!ollama pull mistral:7b\n",
        "!ollama pull falcon:7b\n",
        "\n",
        "print(\" Ollama è pronto e i modelli sono caricati.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fcxfQ3jFzeMH"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3O87DzDYA1kz"
      },
      "source": [
        "AGENTS setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EsAVJ7y8A3jN"
      },
      "outputs": [],
      "source": [
        "from agno.agent import Agent\n",
        "from agno.models.ollama import Ollama\n",
        "\n",
        "local_llm = Ollama(\n",
        "    id=\"qwen2.5:7b-instruct-q4_K_M\",\n",
        "    options={\n",
        "        \"temperature\": 0.01,\n",
        "        \"num_predict\" : 2000, #passare a 12000 per swipe vikidia\n",
        "        \"num_ctx\": 8192,\n",
        "    }\n",
        ")\n",
        "\n",
        "# LINEE GUIDA CONDIVISE\n",
        "COMPLEXITY_GUIDELINES = \"\"\"\n",
        "[COMPLEXITY GUIDELINES]\n",
        "Lexical diversity is measured with MTLD: the text is scanned left-to-right\n",
        "and right-to-left, computing factor lengths—the number of tokens before the\n",
        "running type—token ratio falls below 0.72—and MTLD is the text length di\n",
        "vided by the mean factor length; increasing MTLD means varying lemmas and\n",
        "avoiding repeated phrasings.\n",
        "Lexical density (LD) is evaluated through three quantities. Lexical density\n",
        "is the proportion of content words among all tokens, where content words\n",
        "are tokens tagged as NOUN, VERB, ADJ, or ADV (proper nouns are ex\n",
        "cluded); increasing LD means using more information-bearing words and fewer\n",
        "function-word fillers.\n",
        "Lexical sophistication (LS) measures the proportion of advanced vocabulary in\n",
        "a text by comparing content words against high-frequency vocabulary. Specif\n",
        "ically, LS is calculated as the ratio of sophisticated content-word tokens to\n",
        "total content words. A content word is classified as sophisticated if its lemma\n",
        "does not appear among the 5,000 most frequent English content-word lemmas.\n",
        "increasing LS means choosing more specific, lower-frequency vocabulary while\n",
        "staying faithful to the source facts.\n",
        "Mean Dependency Distance (MDD) reflects the average span between words\n",
        "that depend on each other; higher values arise when the sentence struc\n",
        "ture places modifiers and complements further from their heads (e.g.,\n",
        "fronted clauses, heavy nominal modification, postponed complements, rela\n",
        "tive clauses), increasing structural load. Higher MDD reflects longer, well\n",
        "formed dependencies—e.g., fronted adverbials, heavy nominal modification,\n",
        "postponed complements, relative clauses whose antecedent is distant—thus a\n",
        "greater structural/memory load.\n",
        "Clausal density (CS) reflects how many clauses are packed into each sentence;\n",
        "higher values arise when subordinate, complement, and relative clauses are\n",
        "embedded rather than splitting ideas into multiple simple sentences. Higher\n",
        "CS reflects packaging more propositions per sentence by adding subordinate\n",
        "structures rather than relying on coordination or splitting into simple sen\n",
        "tences.\n",
        "Lexical cohesion (LC) reflects how consistently the text maintains a lexical\n",
        "thread across sentences through repetition and semantic relatedness (e.g., syn\n",
        "onyms or semantically close terms); higher values indicate stronger linking of\n",
        "entities and ideas over the paragraph.\n",
        "Coherence (CoH) reflects how smoothly topics progress between adjacent sen\n",
        "tences; higher values indicate natural transitions, clear connections, and sus\n",
        "tained thematic continuity. Higher CoH indicates that sentences follow one\n",
        "another naturally, with clear thematic continuity and well-signposted transi\n",
        "tions; abrupt topic shifts or loosely linked sentences reduce the score.\n",
        "\"\"\"\n",
        "\n",
        "# STRUTTURA OUTPUT INTERMEDIO\n",
        "from pydantic import BaseModel, Field\n",
        "from typing import List, Optional\n",
        "\n",
        "\n",
        "\n",
        "# WRITER AGENT: Text Complexification Assistant\n",
        "writer_agent = Agent(\n",
        "    name=\"Writer\",\n",
        "    model=local_llm,\n",
        "    description=\"\"\"\n",
        "    [ROLE]\n",
        "    You are a Text Complexification Assistant in a multi-agent framework for text\n",
        "    complexification. You interact with a Critic Assistant that evaluates the complexity\n",
        "    of your outputs and, when needed, sends you short, concrete action plans for revision\n",
        "    in JSON format. Never flip roles. Never try to provide an action plan. Only the Critic\n",
        "    Assistant is allowed to create or modify action plans. You and the Critic Assistant share\n",
        "    a common interest in collaborating to successfully complete the task.\"\"\",\n",
        "    instructions=[\n",
        "        COMPLEXITY_GUIDELINES,\n",
        "        \"\"\"Your task is to rewrite a given source text so that the generated result is more complex\n",
        "        in lexicon, syntax, and discourse complexity, according to the guidelines provided in\n",
        "        [COMPLEXITY GUIDELINES] and satisfying all the constraints in [OBJECTIVES].\n",
        "        When you are given a source text in [SOURCE TEXT] with its current complexity\n",
        "        profile in [TEXT COMPLEXITY PROFILE] and a target complexity profile in [TARGET\n",
        "        COMPLEXITY PROFILE], you must generate a rewritten version whose complexity\n",
        "        measures, as defined in [COMPLEXITY GUIDELINES], satisfy all the constraints defined\n",
        "        in [OBJECTIVES].\n",
        "        When you are given a previous version of your own output in [PREVIOUS TEXT]\n",
        "        together with an [ACTION PLAN] produced by the Critic Assistant, you must apply only\n",
        "        the specified actions in the plan to rewrite [PREVIOUS TEXT].\n",
        "        You must strictly follow the task description, the objectives, the action plan (when given),\n",
        "        and the output format specified in the current prompt. You must output only the rewrit\n",
        "        ten text, as a continuous passage, with no explanations, no metric values, and no meta\n",
        "        commentary. Never mention the multi-agent framework, the Critic Assistant, the guide\n",
        "        lines, or the objectives in your output.\"\"\"\n",
        "    ],\n",
        "    expected_output=\"\"\"Return only the rewritten text, with no additional headings, no metric values, and no meta\n",
        "    text. Do not report any explanations.\"\"\",\n",
        "    markdown=False,\n",
        "    tool_call_limit=0,\n",
        ")\n",
        "\n",
        "# CRITIC AGENT: Evaluation and Action Planning\n",
        "critic_agent = Agent(\n",
        "    name=\"Critic\",\n",
        "    model=local_llm,\n",
        "    description=\"\"\"You are a Critic Assistant in a multi-agent framework for text complexification. You interact with a\n",
        "    Text Complexification Assistant that rewrites texts according to shared complexity guidelines and\n",
        "    objectives. Never flip roles. Never attempt to rewrite the text yourself. Only the Complexification\n",
        "    Assistant is allowed to produce rewritten texts. You and the Complexification Assistant share a common\n",
        "    interest in collaborating to successfully complete the task. You have always access to the original text\n",
        "    in [SOURCE TEXT].\"\"\",\n",
        "    instructions=[\n",
        "        COMPLEXITY_GUIDELINES,\n",
        "        \"\"\"Your task is to review the following information:- The text currently generated by the Complexification Assistant (in [CURRENT])- Its current complexity profile (in [TEXT COMPLEXITY PROFILE])- The target complexity profile (in [TARGET COMPLEXITY PROFILE])- Any additional diagnostics (in [DIAGNOSTICS])- The original text (in [SOURCE TEXT])\n",
        "        Then, produce a concrete ACTION PLAN that helps the Complexification Assistant to rewrite the text\n",
        "        in [CURRENT] so that all the constraints in [OBJECTIVES] are satisfied according to the definitions\n",
        "        provided in [COMPLEXITY GUIDELINES].\"\"\"\n",
        "    ],\n",
        "    expected_output=\"\"\"Your output must always be a single ACTION PLAN in valid JSON format, composed of a few precise,\n",
        "    immediately actionable editing instructions directed to the Complexification Assistant. Each instruction\n",
        "    must clearly indicate what kind of change is needed (lexical, syntactic, or discourse-related) and how it\n",
        "    should move the text towards satisfying the complexity guidelines and objectives.\n",
        "    Your ACTION PLAN should focus on modifications that increase lexical, syntactic, or discourse complex\n",
        "    ity, or that help satisfy length and structural constraints, as defined in [COMPLEXITY GUIDELINES]\n",
        "    and [OBJECTIVES].\n",
        "    You must strictly follow the task description, the guidelines, the objectives, and the required output\n",
        "    format specified in the current prompt. You must output only a single ACTION PLAN in JSON\n",
        "    format, with no rewritten text, no alternative candidate rewrites, and no additional explanations or\n",
        "    meta-commentary outside the JSON structure.\n",
        "    You must respond with only a single JSON object beginning with { and ending with }. Do not rewrite\n",
        "    the text. Do not provide explanations, commentary, or any additional text outside the JSON object.\n",
        "    If all objectives in [OBJECTIVES] are already satisfied, you must output exactly:\n",
        "    {\"status\": \"objectives satisfied\"} and nothing else. Do not include an action plan field in this case.\n",
        "    If at least one objective is not satisfied, you must output a JSON object of the following form:\n",
        "    {\n",
        "        \"status\": \"revision required\",\n",
        "        \"action_plan\": [\n",
        "        {\n",
        "            \"id\": <integer>,\n",
        "            \"type\": \"<lexical | syntactic | discourse | length | mixed>\",\n",
        "            \"target_metrics\": [\"<metric1>\", \"<metric2>\", ...],\n",
        "            \"location\": \"<where to intervene in the CURRENT text>\",\n",
        "            \"instruction\": \"<one concrete, immediately actionable edit>\"\n",
        "            },\n",
        "        ...\n",
        "        ]\n",
        "    }\n",
        "\n",
        "    The status field must be exactly \"revision required\" when you provide an action plan.\n",
        "    The action plan array must contain between 1 and 6 actions.\n",
        "    Each action must specify a single, immediately actionable edit, clearly indicating where to intervene\n",
        "    (for example, “paragraph 2, sentences 3–5”) and what to do concretely (for example, “replace repeated\n",
        "    ’important’ with ’crucial’, ’vital’, ’essential’”).\n",
        "    An example of JSON response, when revision is required is as follows:\n",
        "\n",
        "    {\n",
        "        \"status\": \"revision required\",\n",
        "        \"action_plan\": [\n",
        "        {\n",
        "            \"id\": 1,\n",
        "            \"type\": \"lexical\",\n",
        "            \"target_metrics\": [\"MTLD\", \"LD\"],\n",
        "            \"location\": \"paragraph 1, sentences 2-3\",\n",
        "            \"instruction\": \"Replace the repeated phrase ’very important’ with more\n",
        "            varied expressions such as ’crucial’, ’fundamental’,\n",
        "            and ’pivotal’.\"\n",
        "            }\n",
        "        ]\n",
        "    }\n",
        "    \"\"\",\n",
        "    markdown=False,\n",
        "    tool_call_limit=0,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g7id-K2vkJ64"
      },
      "outputs": [],
      "source": [
        "# Se l'agente risponde, tutto funziona correttamente\n",
        "\n",
        "agent = Agent(\n",
        "    name=\"tester\",\n",
        "    model=local_llm,\n",
        "    description=\"Test di funzionamento del framework\",\n",
        "    instructions=[\"Verify if the system works. answer yes or no\"],\n",
        "    markdown=False,\n",
        "    tool_call_limit=0,\n",
        ")\n",
        "\n",
        "response = agent.run(\"Are you online?\")\n",
        "print(response.content)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dmtZRzWdBEYT"
      },
      "source": [
        "Dominance and Diagnostic logic"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R_zhUHrxBJEp"
      },
      "outputs": [],
      "source": [
        "def check_dominance(curr_metrics, target_metrics, curr_text, complex_text):\n",
        "    # Length constraint: 80% to 120% of the complex text word count\n",
        "    target_words = len(complex_text.split())\n",
        "    curr_words = len(curr_text.split())\n",
        "    length_ok = (0.8 * target_words) <= curr_words <= (1.2 * target_words)\n",
        "\n",
        "    # Metric dominance: every measure >= target\n",
        "    metrics_met = all(curr_metrics[m] >= target_metrics[m] for m in METRICS_ORDER)\n",
        "\n",
        "    # Strict improvement in each category\n",
        "    # Note: Requires comparison with original source_profile\n",
        "\n",
        "    return length_ok and metrics_met\n",
        "\n",
        "def get_diagnostics(curr_metrics, target_metrics, curr_text, complex_text):\n",
        "    below = [m for m in METRICS_ORDER if curr_metrics[m] < target_metrics[m]]\n",
        "\n",
        "    # Categorize missing drivers\n",
        "    missing_drivers = []\n",
        "    if any(m in below for m in ['MTLD', 'LD', 'LS']): missing_drivers.append('lexical')\n",
        "    if any(m in below for m in ['MDD', 'CS']): missing_drivers.append('syntactic')\n",
        "    if any(m in below for m in ['LC', 'CoH']): missing_drivers.append('discourse')\n",
        "\n",
        "    # Length diagnostics\n",
        "    length_issue = None\n",
        "    target_words = len(complex_text.split())\n",
        "    curr_words = len(curr_text.split())\n",
        "    if curr_words < 0.8 * target_words: length_issue = \"Text too short\"\n",
        "    elif curr_words > 1.2 * target_words: length_issue = \"Text too long\"\n",
        "\n",
        "    return {\"below_target\": below, \"drivers_missing\": missing_drivers, \"length_issues\": length_issue}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bi1pYF9xVBYb"
      },
      "source": [
        "Bridge Function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Yl8xdaSHVDSk"
      },
      "outputs": [],
      "source": [
        "def compute_metrics(text):\n",
        "    \"\"\"\n",
        "    Calcola le 7 metriche per una singola stringa di testo.\n",
        "    Utilizza le funzioni già presenti nel notebook.\n",
        "    \"\"\"\n",
        "    if not text or not isinstance(text, str):\n",
        "        # Return default zero values for all metrics if text is empty or invalid\n",
        "        return {m: 0.0 for m in METRICS_ORDER}\n",
        "\n",
        "    # 1. Calcola le misure lessicali (MTLD, LD, LS) e sintattiche (MDD, CS) in un solo passaggio\n",
        "    #    _analyze_text_all gestisce la pipeline Stanza internamente.\n",
        "    all_lex_syn_results = _analyze_text_all(text, lang='en')\n",
        "\n",
        "    # 2. Calcola le misure di discorso (LC, CoH)\n",
        "    #    Queste funzioni richiedono l'oggetto spaCy globale (spacy_nlp).\n",
        "    lc_score = lexical_cohesion_single(text, spacy_nlp)\n",
        "    coh_score = coherence_single(text, spacy_nlp)\n",
        "\n",
        "    # Uniamo tutti i risultati in un dizionario nel formato richiesto dal framework\n",
        "    return {\n",
        "        \"MTLD\": all_lex_syn_results.get(\"MTLD\", 0.0),\n",
        "        \"LD\": all_lex_syn_results.get(\"LD\", 0.0),\n",
        "        \"LS\": all_lex_syn_results.get(\"LS\", 0.0),\n",
        "        \"MDD\": all_lex_syn_results.get(\"MDD\", 0.0),\n",
        "        \"CS\": all_lex_syn_results.get(\"CS\", 0.0),\n",
        "        \"LC\": lc_score,\n",
        "        \"CoH\": coh_score\n",
        "    }"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5KNwgi0sxnmE"
      },
      "source": [
        "Cella di batch sampling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vWi1If0QrDqY"
      },
      "outputs": [],
      "source": [
        "#da utilizzare prima del lancio dell'analisi massiva per verificare un corretto funzionamento della pipeline\n",
        "\n",
        "\"\"\"import json\n",
        "import pandas as pd\n",
        "from datetime import datetime\n",
        "from pydantic import BaseModel, Field\n",
        "from typing import List, Optional\n",
        "\n",
        "# 1. DEFINIZIONE SCHEMI PYDANTIC\n",
        "class ActionItem(BaseModel):\n",
        "    id: int\n",
        "    type: str = Field(description=\"lexical | syntactic | discourse | length | mixed\")\n",
        "    target_metrics: List[str]\n",
        "    location: str\n",
        "    instruction: str\n",
        "\n",
        "class CriticResponse(BaseModel):\n",
        "    status: str = Field(description=\"revision required | objectives satisfied\")\n",
        "    action_plan: Optional[List[ActionItem]] = None\n",
        "\n",
        "# 2. Caricamento Dataset\n",
        "df = pd.read_csv(\"CLEANED_final_complexity_ose_adv_ele.csv\", sep='\\t')\n",
        "\n",
        "MODELS_TO_TEST = [\"llama3.1:8b\"]\n",
        "NUM_ROWS_TO_TEST = 5\n",
        "MAX_ITERATIONS = 10\n",
        "METRICS_ORDER = ['MTLD', 'LD', 'LS', 'MDD', 'CS', 'LC', 'CoH']\n",
        "\n",
        "def run_benchmark_session(row_index, df, output_file, model_id, max_k=10):\n",
        "    row = df.iloc[row_index]\n",
        "    source_text = row['Simple']\n",
        "    complex_ref_text = row['Complex']\n",
        "    target_profile = [round(float(row[f'Complex_{m}']), 3) for m in METRICS_ORDER]\n",
        "    target_map = {m: float(row[f'Complex_{m}']) for m in METRICS_ORDER}\n",
        "\n",
        "    ref_word_count = int(row['Complex_word_count'])\n",
        "    min_w, max_w = int(ref_word_count * 0.8), int(ref_word_count * 1.2)\n",
        "\n",
        "    run_id = f\"exp_{model_id.replace(':', '_')}_row_{row_index}\"\n",
        "    current_text = source_text\n",
        "    last_critic_json = {}\n",
        "    previous_text_profile = [round(float(row[f'Simple_{m}']), 3) for m in METRICS_ORDER]\n",
        "\n",
        "    for k in range(max_k + 1):\n",
        "        print(f\"       Iterazione {k}...\", end=\" \")\n",
        "        mode = \"bootstrap\" if k == 0 else \"refinement\"\n",
        "\n",
        "        # --- FASE 1: WRITER AGENT ---\n",
        "        if k == 0:\n",
        "            instruction = (\n",
        "                f\"[TASK]: Rewrite [SOURCE TEXT] to match [TARGET COMPLEXITY PROFILE].\\n\"\n",
        "                f\"[TARGET COMPLEXITY PROFILE]: {target_profile}\\n\"\n",
        "                f\"[SOURCE TEXT]: {source_text}\\n\"\n",
        "                f\"[OBJECTIVES]: Word count in [{min_w}, {max_w}].\\n\"\n",
        "                f\"[OUTPUT FORMAT]: Return only the rewritten text.\"\n",
        "            )\n",
        "            input_for_writer = source_text\n",
        "        else:\n",
        "            plan_to_pass = json.dumps(last_critic_json, indent=2)\n",
        "            instruction = (\n",
        "                f\"Rewrite [PREVIOUS TEXT] applying ONLY the 'action plan' in [ACTION PLAN].\\n\"\n",
        "                f\"[ACTION PLAN]: {plan_to_pass}\\n\"\n",
        "                f\"[PREVIOUS TEXT]: {current_text}\\n\"\n",
        "            )\n",
        "            input_for_writer = current_text\n",
        "            print(f\"\\n      [DEBUG WRITER] Action Plan inviato:\\n{plan_to_pass}\")\n",
        "\n",
        "        writer_res = writer_agent.run(instruction)\n",
        "        rewritten_text = writer_res.content.strip()\n",
        "\n",
        "        if k > 0 and rewritten_text == current_text:\n",
        "            print(\"       WARNING: Testo identico rilevato.\")\n",
        "\n",
        "        # FASE 2: SCORING-\n",
        "        rewritten_metrics = compute_metrics(rewritten_text)\n",
        "        rewritten_profile = [round(rewritten_metrics[m], 3) for m in METRICS_ORDER]\n",
        "        diag_text = get_diagnostics(rewritten_metrics, target_map, rewritten_text, complex_ref_text)\n",
        "\n",
        "        # FASE 3: CRITIC AGENT\n",
        "        critic_instruction = (\n",
        "            f\"[TASK]: Review [CURRENT] against [TARGET COMPLEXITY PROFILE].\\n\"\n",
        "            f\"[CURRENT]: {rewritten_text}\\n\"\n",
        "            f\"[DIAGNOSTICS]: {diag_text}\\n\"\n",
        "        )\n",
        "\n",
        "        try:\n",
        "\n",
        "            critic_res = critic_agent.run(critic_instruction, response_model=CriticResponse)\n",
        "\n",
        "            # Debug per vedere l'output reale\n",
        "            print(f\"      [DEBUG CRITIC] RAW: {str(critic_res.content)[:100]}...\")\n",
        "\n",
        "            if isinstance(critic_res.content, CriticResponse):\n",
        "                critic_output_obj = critic_res.content\n",
        "            else:\n",
        "                # Gestione stringa in caso Agno non faccia l'auto-cast\n",
        "                import re\n",
        "                clean_json = re.sub(r'```json\\s*|```', '', str(critic_res.content)).strip()\n",
        "                critic_output_obj = CriticResponse(**json.loads(clean_json))\n",
        "\n",
        "            last_critic_json = critic_output_obj.model_dump(exclude_none=True)\n",
        "            print(f\"      [DEBUG CRITIC] Action Plan generato con successo.\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"      [DEBUG CRITIC] ERRORE: {str(e)}\")\n",
        "            last_critic_json = {\n",
        "                \"status\": \"revision required\",\n",
        "                \"action_plan\": [{\"id\": 1, \"type\": \"mixed\", \"target_metrics\": [], \"location\": \"all\", \"instruction\": \"Increase complexity.\"}]\n",
        "            }\n",
        "\n",
        "        # LOGGING E AGGIORNAMENTO STATO\n",
        "        log_entry = {\n",
        "            \"run_id\": run_id, \"iteration\": k, \"timestamp\": datetime.now().isoformat(),\n",
        "            \"source_text\": source_text, \"target_complexity_profile\": target_profile,\n",
        "            \"complexification_output\": {\"rewritten_text\": rewritten_text, \"rewritten_text_profile\": rewritten_profile},\n",
        "            \"critic_output\": last_critic_json\n",
        "        }\n",
        "        output_file.write(json.dumps(log_entry) + \"\\n\")\n",
        "        output_file.flush()\n",
        "\n",
        "        current_text = rewritten_text\n",
        "        previous_text_profile = rewritten_profile\n",
        "        status = last_critic_json.get(\"status\", \"revision required\")\n",
        "        print(f\"      Status: {status}\")\n",
        "\n",
        "        if \"satisfied\" in status.lower():\n",
        "            print(\"      Successo raggiunto.\")\n",
        "            break\n",
        "\n",
        "# --- LOOP DI TEST ---\n",
        "for model_id in MODELS_TO_TEST:\n",
        "    print(f\"\\n CAMBIO MODELLO DI ALIMENTAZIONE: {model_id}\")\n",
        "    new_llm = Ollama(id=model_id, options={\"temperature\": 0.01, \"num_predict\": 2000, \"num_ctx\": 8192})\n",
        "    writer_agent.model = new_llm\n",
        "    critic_agent.model = new_llm\n",
        "\n",
        "    output_filename = f\"trace_{model_id.replace(':', '_')}.jsonl\"\n",
        "    with open(output_filename, \"w\", encoding=\"utf-8\") as f_trace:\n",
        "        for i in range(NUM_ROWS_TO_TEST):\n",
        "            print(f\"   Test riga {i}...\")\n",
        "            run_benchmark_session(i, df, f_trace, model_id, max_k=MAX_ITERATIONS)\n",
        "\n",
        "print(\"\\n TUTTI I TEST COMPLETATI.\")\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1UUAKjOmzbX-"
      },
      "source": [
        "IN CASO DI ARRESTO MANUALE DELLA CELLA, RIAVVIARE OLLAMA DA QUI"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Az-xpK9LrmF6"
      },
      "outputs": [],
      "source": [
        "env = os.environ.copy()\n",
        "env[\"OLLAMA_HOST\"] = \"127.0.0.1:11434\"\n",
        "\n",
        "print(\" Avvio server Ollama...\")\n",
        "with open(\"ollama.log\", \"w\") as f:\n",
        "    subprocess.Popen([\"ollama\", \"serve\"], stdout=f, stderr=f, env=env)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Wb0p37cxyLP"
      },
      "source": [
        "Analisi massiva per campionamento manuale e automatico"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Mk9yV8TFzJea"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "\n",
        "# --- 0. MOUNT DRIVE E SETUP PERCORSI ---\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Definisci la cartella di output sul tuo Drive\n",
        "# Assicurati che questa cartella esista o creala manualmente\n",
        "OUTPUT_FOLDER = \"/content/drive/MyDrive/Tesi_Complexity_Results\"\n",
        "if not os.path.exists(OUTPUT_FOLDER):\n",
        "    os.makedirs(OUTPUT_FOLDER)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gbBHPu4wzdTX"
      },
      "outputs": [],
      "source": [
        "# Testiamo il montaggio del drive. Eseguiamo un remount per sicurezza nonostante sia ridondante\n",
        "\n",
        "\"\"\"import os\n",
        "import json\n",
        "from google.colab import drive\n",
        "\n",
        "# Montaggio Drive\n",
        "print(\" Montaggio Google Drive...\")\n",
        "drive.mount('/content/drive', force_remount=True)\n",
        "\n",
        "# Configurazione percorsi\n",
        "# Assicuriamoci che il nome della cartella e quello del file siano distinti\n",
        "OUTPUT_FOLDER = \"/content/drive/MyDrive/Tesi_Complexity_Results\"\n",
        "test_file_name = \"test_salvataggio_v2.jsonl\"\n",
        "test_file_path = os.path.join(OUTPUT_FOLDER, test_file_name)\n",
        "\n",
        "# Creazione cartella se non esiste\n",
        "if not os.path.exists(OUTPUT_FOLDER):\n",
        "    os.makedirs(OUTPUT_FOLDER)\n",
        "    print(f\" Cartella creata: {OUTPUT_FOLDER}\")\n",
        "else:\n",
        "    print(f\" Cartella già esistente: {OUTPUT_FOLDER}\")\n",
        "\n",
        "# Test di scrittura (Specificando il file, non la cartella)\n",
        "test_data = {\n",
        "    \"test_id\": \"check_final\",\n",
        "    \"timestamp\": \"2024-05-20T10:30:00\",\n",
        "    \"message\": \"Test di scrittura corretto per la tesi\"\n",
        "}\n",
        "\n",
        "print(f\" Scrittura file di test in: {test_file_path}...\")\n",
        "try:\n",
        "    # Verifichiamo che il path non sia una cartella prima di scrivere\n",
        "    if os.path.isdir(test_file_path):\n",
        "        print(f\" Errore: {test_file_path} è una directory, non un file!\")\n",
        "    else:\n",
        "        with open(test_file_path, \"w\", encoding=\"utf-8\") as f:\n",
        "            f.write(json.dumps(test_data) + \"\\n\")\n",
        "        print(\" Scrittura completata con successo.\")\n",
        "except Exception as e:\n",
        "    print(f\" Errore durante la scrittura: {e}\")\n",
        "\n",
        "# Verifica di lettura\n",
        "print(\" Verifica integrità dati...\")\n",
        "try:\n",
        "    with open(test_file_path, \"r\", encoding=\"utf-8\") as f:\n",
        "        read_data = json.loads(f.readline())\n",
        "        print(f\" Test superato! Dati letti: {read_data}\")\n",
        "except Exception as e:\n",
        "    print(f\" Errore durante la lettura: {e}\")\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ygvgmx4Dx44G"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import pandas as pd\n",
        "import os\n",
        "import re\n",
        "from datetime import datetime\n",
        "from pydantic import BaseModel, Field\n",
        "from typing import List, Optional\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "# --- SCHEMI PYDANTIC ---\n",
        "class ActionItem(BaseModel):\n",
        "    id: int\n",
        "    type: str = Field(description=\"lexical | syntactic | discourse | length | mixed\")\n",
        "    target_metrics: List[str]\n",
        "    location: str\n",
        "    instruction: str\n",
        "\n",
        "class CriticResponse(BaseModel):\n",
        "    status: str = Field(description=\"revision required | objectives satisfied\")\n",
        "    action_plan: Optional[List[ActionItem]] = None\n",
        "\n",
        "# CONFIGURAZIONE DATASET E PARAMETRI\n",
        "DATASET_FILES = [\n",
        "    \"CLEANED_final_complexity_ose_adv_ele.csv\",\n",
        "    \"CLEANED_final_complexity_ose_adv_int.csv\",\n",
        "    \"CLEANED_final_complexity_swipe.csv\",\n",
        "    \"CLEANED_final_complexity_vikidia.csv\"\n",
        "]\n",
        "\n",
        "MODELS_TO_TEST = [\"llama3.1:8b\", \"mistral:7b\", \"qwen2.5:7b-instruct-q4_K_M\", \"falcon:7b\"]\n",
        "MAX_ITERATIONS = 10\n",
        "METRICS_ORDER = ['MTLD', 'LD', 'LS', 'MDD', 'CS', 'LC', 'CoH']\n",
        "# OUTPUT_FOLDER = \"/content/drive/MyDrive/Tesi/Risultati\" # O qualsiasi cartella si sia indicata in precedenza\n",
        "\n",
        "def run_benchmark_session(row_index, df, output_file, model_id, max_k=10):\n",
        "    row = df.iloc[row_index]\n",
        "    source_text = row['Simple']\n",
        "    complex_ref_text = row['Complex']\n",
        "    target_profile = [round(float(row[f'Complex_{m}']), 3) for m in METRICS_ORDER]\n",
        "    target_map = {m: float(row[f'Complex_{m}']) for m in METRICS_ORDER}\n",
        "\n",
        "    ref_word_count = int(row['Complex_word_count'])\n",
        "    min_w, max_w = int(ref_word_count * 0.8), int(ref_word_count * 1.2)\n",
        "\n",
        "    run_id = f\"exp_{model_id.replace(':', '_')}_row_{row_index}\"\n",
        "    current_text = source_text\n",
        "    last_critic_json = {}\n",
        "\n",
        "    for k in range(max_k + 1):\n",
        "        print(f\"       Iterazione {k}...\", end=\" \")\n",
        "\n",
        "        # FASE 1: WRITER AGENT\n",
        "        if k == 0:\n",
        "            instruction = (f\"[TASK]: Rewrite [SOURCE TEXT] to match profile {target_profile}.\\n\"\n",
        "                           f\"[SOURCE TEXT]: {source_text}\\n\"\n",
        "                           f\"[OBJECTIVES]: Word count in [{min_w}, {max_w}].\\n\"\n",
        "                           f\"[OUTPUT FORMAT]: Return only the rewritten text.\")\n",
        "        else:\n",
        "            plan_to_pass = json.dumps(last_critic_json, indent=2)\n",
        "            print(f\"\\n      [DEBUG WRITER] Action Plan inviato:\\n{plan_to_pass}\")\n",
        "            instruction = (f\"Rewrite [PREVIOUS TEXT] applying ONLY the 'action plan' in [ACTION PLAN].\\n\"\n",
        "                           f\"[ACTION PLAN]: {plan_to_pass}\\n\"\n",
        "                           f\"[PREVIOUS TEXT]: {current_text}\\n\"\n",
        "                           f\"STRICT RULE: Do not truncate. Return FULL text.\")\n",
        "\n",
        "        writer_res = writer_agent.run(instruction)\n",
        "        rewritten_text = writer_res.content.strip()\n",
        "\n",
        "        if k > 0 and rewritten_text == current_text:\n",
        "            print(\"       WARNING: Testo identico rilevato.\")\n",
        "\n",
        "        # FASE 2: SCORING\n",
        "        rewritten_metrics = compute_metrics(rewritten_text)\n",
        "        rewritten_profile = [round(rewritten_metrics[m], 3) for m in METRICS_ORDER]\n",
        "        diag_text = get_diagnostics(rewritten_metrics, target_map, rewritten_text, complex_ref_text)\n",
        "\n",
        "        # FASE 3: CRITIC AGENT\n",
        "        critic_instruction = (f\"[TASK]: Review [CURRENT] against [TARGET PROFILE].\\n\"\n",
        "                              f\"[CURRENT]: {rewritten_text}\\n\"\n",
        "                              f\"[DIAGNOSTICS]: {diag_text}\")\n",
        "\n",
        "        try:\n",
        "            critic_res = critic_agent.run(critic_instruction, response_model=CriticResponse)\n",
        "            print(f\"      [DEBUG CRITIC] RAW: {str(critic_res.content)[:80]}...\")\n",
        "\n",
        "            if isinstance(critic_res.content, CriticResponse):\n",
        "                critic_output_obj = critic_res.content\n",
        "            else:\n",
        "                clean_json = re.sub(r'```json\\s*|```', '', str(critic_res.content)).strip()\n",
        "                critic_output_obj = CriticResponse(**json.loads(clean_json))\n",
        "\n",
        "            last_critic_json = critic_output_obj.model_dump(exclude_none=True)\n",
        "            print(f\"      [DEBUG CRITIC] Action Plan generato con successo.\")\n",
        "        except Exception as e:\n",
        "            print(f\"      ❌ [DEBUG CRITIC] ERRORE: {str(e)}\")\n",
        "            last_critic_json = {\"status\": \"revision required\", \"action_plan\": [{\"id\": 1, \"instruction\": \"Increase complexity.\"}]}\n",
        "\n",
        "        # --- LOGGING ---\n",
        "        log_entry = {\n",
        "            \"run_id\": run_id, \"iteration\": k, \"timestamp\": datetime.now().isoformat(),\n",
        "            \"source_text\": source_text, \"human_reference\": complex_ref_text,\n",
        "            \"complexification_output\": {\"rewritten_text\": rewritten_text, \"rewritten_text_profile\": rewritten_profile},\n",
        "            \"critic_output\": last_critic_json\n",
        "        }\n",
        "        output_file.write(json.dumps(log_entry) + \"\\n\")\n",
        "        output_file.flush()\n",
        "\n",
        "        current_text = rewritten_text\n",
        "        status = last_critic_json.get(\"status\", \"revision required\")\n",
        "        print(f\"      Status: {status}\")\n",
        "\n",
        "        if \"satisfied\" in status.lower():\n",
        "            print(\"       Successo raggiunto.\")\n",
        "            break\n",
        "\n",
        "# LOOP DI ESECUZIONE COMPARATIVA\n",
        "pbar_global = tqdm(total=len(DATASET_FILES) * len(MODELS_TO_TEST), desc=\" Esperimento\")\n",
        "\n",
        "for ds_path in DATASET_FILES:\n",
        "    df_current = pd.read_csv(ds_path, sep='\\t')\n",
        "    ds_name = ds_path.replace('.csv', '')\n",
        "    is_heavy = any(x in ds_path.lower() for x in [\"swipe\", \"vikidia\"])\n",
        "\n",
        "    # COMPARAZIONE DIRETTA: Stesse righe per ogni modello\n",
        "    target_range = range(0, 3) if is_heavy else range(0, 12)\n",
        "\n",
        "    for model_id in MODELS_TO_TEST:\n",
        "        print(f\"\\n MODELLO: {model_id} | DATASET: {ds_name}\")\n",
        "\n",
        "        # Configurazione specidica per il peso del dataset\n",
        "        target_predict = 12000 if is_heavy else 2500\n",
        "        target_ctx = 16384 if is_heavy else 8192\n",
        "        new_llm = Ollama(id=model_id, options={\"temperature\": 0.01, \"num_predict\": target_predict, \"num_ctx\": target_ctx})\n",
        "        writer_agent.model = new_llm\n",
        "        critic_agent.model = new_llm\n",
        "\n",
        "        output_filename = os.path.join(OUTPUT_FOLDER, f\"trace_{ds_name}_{model_id.replace(':', '_')}.jsonl\")\n",
        "\n",
        "        # Checkpoint: quante righe sono già state salvate?\n",
        "        done_indices = []\n",
        "        if os.path.exists(output_filename):\n",
        "            with open(output_filename, \"r\", encoding=\"utf-8\") as f_check:\n",
        "                for line in f_check:\n",
        "                    try:\n",
        "                        data = json.loads(line)\n",
        "                        idx = int(data['run_id'].split('_')[-1])\n",
        "                        if idx not in done_indices: done_indices.append(idx)\n",
        "                    except: continue\n",
        "\n",
        "        with open(output_filename, \"a\", encoding=\"utf-8\") as f_trace:\n",
        "            for i in target_range:\n",
        "                if i in done_indices:\n",
        "                    print(f\"   Riga {i} già completata. Salto.\")\n",
        "                    continue\n",
        "                print(f\"   Elaborazione riga {i}...\")\n",
        "                run_benchmark_session(i, df_current, f_trace, model_id, max_k=MAX_ITERATIONS)\n",
        "\n",
        "        pbar_global.update(1)\n",
        "\n",
        "print(f\"\\n TUTTI I TEST COMPLETATI. Risultati pronti in: {OUTPUT_FOLDER}\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}