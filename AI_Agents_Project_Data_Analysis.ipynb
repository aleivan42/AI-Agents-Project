{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Analisi preliminare dei dati ottenuti da data_treatment"
      ],
      "metadata": {
        "id": "nlV7S1EC5rLS"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vvx0Gu-R32wF"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import os\n",
        "import sys\n",
        "import csv\n",
        "\n",
        "# Configurazione file\n",
        "file_names = [\n",
        "    \"final_complexity_vikidia.csv\",\n",
        "    \"final_complexity_swipe.csv\",\n",
        "    \"final_complexity_ose_adv_int.csv\",\n",
        "    \"final_complexity_ose_adv_ele.csv\"\n",
        "]\n",
        "\n",
        "metrics = ['MTLD', 'LD', 'LS', 'MDD', 'CS', 'LC', 'CoH']\n",
        "\n",
        "# Creazione cartelle di output\n",
        "os.makedirs(\"output/discarded_ids\", exist_ok=True)\n",
        "os.makedirs(\"output/plots\", exist_ok=True)\n",
        "os.makedirs(\"output/cleaned_data\", exist_ok=True)\n",
        "\n",
        "def run_analysis():\n",
        "    all_summaries = []\n",
        "\n",
        "    max_int = sys.maxsize\n",
        "    while True:\n",
        "        try:\n",
        "            csv.field_size_limit(max_int)\n",
        "            break\n",
        "        except OverflowError:\n",
        "            max_int = int(max_int / 2)\n",
        "\n",
        "    for file_path in file_names:\n",
        "        if not os.path.exists(file_path):\n",
        "            continue\n",
        "\n",
        "        try:\n",
        "            df = pd.read_csv(file_path, sep='\\t', engine='python')\n",
        "            print(f\"\\n>>> Analizzando: {file_path}\")\n",
        "        except pd.errors.ParserError as e:\n",
        "            print(f\"Errore di parsing nel file '{file_path}': {e}. Prova a controllare la riga 446 per virgolette non chiuse o formattazione errata.\")\n",
        "            continue # Salta il file corrente e passa al successivo\n",
        "\n",
        "        #Conteggio Parole e Token\n",
        "        for col in ['Simple', 'Complex']:\n",
        "            if col in df.columns:\n",
        "                # Calcolo lunghezza parole\n",
        "                df[f'{col}_word_count'] = df[col].apply(lambda x: len(str(x).split()))\n",
        "                # Calcolo token (rimozione punteggiatura base per stima)\n",
        "                df[f'{col}_token_count'] = df[col].apply(lambda x: len(str(x).replace('.', ' ').split()))\n",
        "\n",
        "        # Analisi Dominanza (MDD)\n",
        "        violations = df[df['Simple_MDD'] >= df['Complex_MDD']]\n",
        "\n",
        "        # Salvataggio ID scartati (usiamo l'indice del dataframe come ID)\n",
        "        txt_name = f\"output/discarded_ids/discarded_{file_path.replace('.csv', '.txt')}\"\n",
        "        violations.index.to_series().to_csv(txt_name, index=False, header=False)\n",
        "\n",
        "        # Creazione Dataset Pulito\n",
        "        # Modificato per mantenere l'indice originale nel file CSV pulito\n",
        "        df_cleaned = df[df['Simple_MDD'] < df['Complex_MDD']]\n",
        "        df_cleaned.to_csv(f\"output/cleaned_data/CLEANED_{file_path}\", index=True, sep='\\t')\n",
        "\n",
        "        # Grafico di Distribuzione MDD\n",
        "        plt.figure(figsize=(10, 6))\n",
        "        sns.kdeplot(df['Simple_MDD'], fill=True, label='Simple MDD', color='blue')\n",
        "        sns.kdeplot(df['Complex_MDD'], fill=True, label='Complex MDD', color='orange')\n",
        "        plt.title(f'Distribuzione MDD - {file_path}')\n",
        "        plt.legend()\n",
        "        plt.savefig(f\"output/plots/dist_{file_path.replace('.csv', '.png')}\")\n",
        "        plt.close()\n",
        "\n",
        "        # Raccolta Statistiche\n",
        "        all_summaries.append({\n",
        "            \"File\": file_path,\n",
        "            \"Righe Totali\": len(df),\n",
        "            \"Scarti (Violazioni)\": len(violations),\n",
        "            \"Media Parole (S)\": f\"{df['Simple_word_count'].mean():.1f}\",\n",
        "            \"Media Parole (C)\": f\"{df['Complex_word_count'].mean():.1f}\",\n",
        "            \"Media Token (S)\": f\"{df['Simple_token_count'].mean():.1f}\",\n",
        "            \"Media Token (C)\": f\"{df['Complex_token_count'].mean():.1f}\",\n",
        "            \"Mediana Token (S)\": f\"{df['Simple_token_count'].median():.1f}\",\n",
        "            \"Mediana Token (C)\": f\"{df['Complex_token_count'].median():.1f}\",\n",
        "            \"Token più lunghi (S)\": f\"{df['Simple_token_count'].max()}\",\n",
        "            \"Token più lunghi (C)\": f\"{df['Complex_token_count'].max()}\",\n",
        "            \"Dominanza OK\": f\"{(1 - len(violations)/len(df))*100:.2f}%\"\n",
        "        })\n",
        "\n",
        "    # Visualizzazione Tabella Finale\n",
        "    summary_df = pd.DataFrame(all_summaries)\n",
        "    print(\"\\n--- RIEPILOGO STATISTICO FINALE ---\")\n",
        "    print(summary_df.to_string(index=False))\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    run_analysis()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Once finished the work on the pipeline, use the JSONL files here to get the cosine similarity."
      ],
      "metadata": {
        "id": "WYvI5b-BTkbJ"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "55ee392e"
      },
      "source": [
        "import pandas as pd\n",
        "import json\n",
        "import os\n",
        "\n",
        "jsonl_files = [\n",
        "    '/content/trace_CLEANED_final_complexity_ose_adv_ele_falcon_7b.jsonl',\n",
        "    '/content/trace_CLEANED_final_complexity_ose_adv_ele_llama3.1_8b.jsonl',\n",
        "    '/content/trace_CLEANED_final_complexity_ose_adv_ele_mistral_7b.jsonl',\n",
        "    '/content/trace_CLEANED_final_complexity_ose_adv_ele_qwen2.5_7b-instruct-q4_K_M.jsonl',\n",
        "    '/content/trace_CLEANED_final_complexity_ose_adv_int_falcon_7b.jsonl',\n",
        "    '/content/trace_CLEANED_final_complexity_ose_adv_int_llama3.1_8b.jsonl',\n",
        "    '/content/trace_CLEANED_final_complexity_ose_adv_int_mistral_7b.jsonl',\n",
        "    '/content/trace_CLEANED_final_complexity_ose_adv_int_qwen2.5_7b-instruct-q4_K_M.jsonl'\n",
        "]\n",
        "\n",
        "all_data = []\n",
        "\n",
        "print(\"Caricamento dei file JSONL...\")\n",
        "for file_path in jsonl_files:\n",
        "    if os.path.exists(file_path):\n",
        "        print(f\"  Caricamento: {file_path}\")\n",
        "        with open(file_path, 'r', encoding='utf-8') as f:\n",
        "            for line in f:\n",
        "                record = json.loads(line)\n",
        "                record['original_jsonl_file'] = os.path.basename(file_path) # Add original filename\n",
        "                all_data.append(record)\n",
        "    else:\n",
        "        print(f\"  File non trovato e skippato: {file_path}\")\n",
        "\n",
        "df = pd.DataFrame(all_data)\n",
        "\n",
        "print(f\"\\nCaricamento completato. Numero totale di record: {len(df)}\")\n",
        "print(\"Prime 5 righe del DataFrame combinato 'df':\")\n",
        "display(df.head())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "b5d45982"
      },
      "source": [
        "import json\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "results = []\n",
        "\n",
        "# Iteriamo per ogni run_id nel DataFrame combinato\n",
        "for run_id, group in df.groupby('run_id'):\n",
        "    try:\n",
        "        # 1. Recupera Source Text (dall'iterazione 0)\n",
        "        source_row = group[group['iteration'] == 0]\n",
        "        source_text = source_row.iloc[0].get('source_text', '') if not source_row.empty else \"\"\n",
        "\n",
        "        # 2. Recupera Rewritten Text (dall'ultima iterazione disponibile)\n",
        "        max_iter = group['iteration'].max()\n",
        "        target_row = group[group['iteration'] == max_iter]\n",
        "\n",
        "        rewritten_text = \"\"\n",
        "        if not target_row.empty:\n",
        "            complexification_output_raw = target_row.iloc[0].get('complexification_output', '')\n",
        "\n",
        "            if isinstance(complexification_output_raw, str):\n",
        "                try:\n",
        "                    complexification_output_dict = json.loads(complexification_output_raw)\n",
        "                    rewritten_text = complexification_output_dict.get('rewritten_text', '')\n",
        "                except json.JSONDecodeError:\n",
        "                    # Handle cases where the string is not valid JSON\n",
        "                    pass\n",
        "            elif isinstance(complexification_output_raw, dict):\n",
        "                rewritten_text = complexification_output_raw.get('rewritten_text', '')\n",
        "\n",
        "        # 3. Pulizia e verifica (Gestione NaN di Pandas)\n",
        "        if pd.isna(source_text): source_text = \"\"\n",
        "        if pd.isna(rewritten_text): rewritten_text = \"\"\n",
        "\n",
        "        # Verifica finale che ci sia testo\n",
        "        if not source_text or not rewritten_text:\n",
        "            results.append({\n",
        "                'Run ID': run_id,\n",
        "                'Iterations': max_iter,\n",
        "                'Cosine Similarity': 0.0,\n",
        "                'Source Text': source_text,\n",
        "                'Rewritten Text': rewritten_text,\n",
        "                'Note': f\"Testo mancante (Source: {bool(source_text)}, Rewritten: {bool(rewritten_text)})\"\n",
        "            })\n",
        "            continue\n",
        "\n",
        "        # 4. Calcolo Cosine Similarity\n",
        "        tfidf_vectorizer = TfidfVectorizer()\n",
        "        tfidf_matrix = tfidf_vectorizer.fit_transform([str(source_text), str(rewritten_text)])\n",
        "        similarity_score = cosine_similarity(tfidf_matrix[0:1], tfidf_matrix[1:2])[0][0]\n",
        "\n",
        "        results.append({\n",
        "            'Run ID': run_id,\n",
        "            'Iterations': max_iter,\n",
        "            'Cosine Similarity': round(similarity_score, 4),\n",
        "            'Source Text': source_text,\n",
        "            'Rewritten Text': rewritten_text,\n",
        "            'Note': 'OK'\n",
        "        })\n",
        "\n",
        "    except Exception as e:\n",
        "        results.append({\n",
        "            'Run ID': run_id,\n",
        "            'Iterations': group['iteration'].max(),\n",
        "            'Cosine Similarity': 0.0,\n",
        "            'Source Text': source_text,\n",
        "            'Rewritten Text': rewritten_text,\n",
        "            'Note': f\"Errore: {str(e)}\"\n",
        "        })\n",
        "\n",
        "# Output\n",
        "results_df = pd.DataFrame(results)\n",
        "\n",
        "# Function to truncate text for display\n",
        "def truncate_text(text, length=100):\n",
        "    return (text[:length] + '...') if len(text) > length else text\n",
        "\n",
        "# Apply truncation for display purposes\n",
        "display_df = results_df.copy()\n",
        "display_df['Source Text'] = display_df['Source Text'].apply(truncate_text)\n",
        "display_df['Rewritten Text'] = display_df['Rewritten Text'].apply(truncate_text)\n",
        "\n",
        "print(display_df.to_markdown(index=False))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Salva i risultati"
      ],
      "metadata": {
        "id": "EHMFn0Q_hKOi"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "067289ca"
      },
      "source": [
        "\n",
        "output_detailed_results_csv_path = 'detailed_cosine_similarity_results.csv'\n",
        "\n",
        "results_df.to_csv(output_detailed_results_csv_path, index=False)\n",
        "\n",
        "print(f\"I risultati dettagliati dell'analisi sono stati salvati con successo in '{output_detailed_results_csv_path}'\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Medie similarità per file"
      ],
      "metadata": {
        "id": "KUdB-jJnfE1y"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "69ea1015"
      },
      "source": [
        "# Estrai il nome del file da 'Run ID' per raggruppare i risultati\n",
        "results_df['File Name'] = results_df['Run ID'].apply(lambda x: x.split('_row_')[0].replace('exp_', '') + '.jsonl')\n",
        "\n",
        "# Calcola la media della similarità coseno per ogni file\n",
        "average_cosine_similarity_per_file = results_df.groupby('File Name')['Cosine Similarity'].mean().reset_index()\n",
        "\n",
        "print(\"Similarità Cosine Media per File:\")\n",
        "print(average_cosine_similarity_per_file.to_markdown(index=False))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9a9c7bae"
      },
      "source": [
        "import json\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "metrics = ['MTLD', 'LD', 'LS', 'MDD', 'CS', 'LC', 'CoH']\n",
        "results_with_metrics = []\n",
        "\n",
        "# Iteriamo per ogni combinazione unica di file originale e run_id\n",
        "# This ensures that entries from different files but with the same 'run_id' are treated separately\n",
        "for (original_jsonl_file, run_id_in_file), group in df.groupby(['original_jsonl_file', 'run_id']):\n",
        "    # Create a unique identifier for this specific row across all files\n",
        "    unique_run_identifier = f\"{original_jsonl_file}_{run_id_in_file}\"\n",
        "    try:\n",
        "        # Recupera Source Text (dall'iterazione 0)\n",
        "        source_row = group[group['iteration'] == 0]\n",
        "        source_text = source_row.iloc[0].get('source_text', '') if not source_row.empty else \"\"\n",
        "\n",
        "        # Recupera Rewritten Text (dall'ultima iterazione disponibile)\n",
        "        max_iter = group['iteration'].max()\n",
        "        target_row = group[group['iteration'] == max_iter]\n",
        "\n",
        "        rewritten_text = \"\"\n",
        "        if not target_row.empty:\n",
        "            complexification_output_raw = target_row.iloc[0].get('complexification_output', '')\n",
        "            if isinstance(complexification_output_raw, str):\n",
        "                try:\n",
        "                    complexification_output_dict = json.loads(complexification_output_raw)\n",
        "                    rewritten_text = complexification_output_dict.get('rewritten_text', '')\n",
        "                except json.JSONDecodeError:\n",
        "                    pass\n",
        "            elif isinstance(complexification_output_raw, dict):\n",
        "                rewritten_text = complexification_output_raw.get('rewritten_text', '')\n",
        "\n",
        "        # Pulizia e verifica (Gestione NaN di Pandas)\n",
        "        if pd.isna(source_text): source_text = \"\"\n",
        "        if pd.isna(rewritten_text): rewritten_text = \"\"\n",
        "\n",
        "        # Calcolo Cosine Similarity\n",
        "        similarity_score = 0.0\n",
        "        if source_text and rewritten_text:\n",
        "            tfidf_vectorizer = TfidfVectorizer()\n",
        "            texts_to_vectorize = [str(source_text), str(rewritten_text)]\n",
        "            if all(t.strip() for t in texts_to_vectorize):\n",
        "                tfidf_matrix = tfidf_vectorizer.fit_transform(texts_to_vectorize)\n",
        "                if tfidf_matrix.shape[0] == 2 and tfidf_matrix.shape[1] > 0:\n",
        "                    similarity_score = cosine_similarity(tfidf_matrix[0:1], tfidf_matrix[1:2])[0][0]\n",
        "\n",
        "        # Estrazione e confronto delle metriche\n",
        "        source_profile = None\n",
        "        target_profile = None\n",
        "\n",
        "        if not source_row.empty:\n",
        "            source_output_raw = source_row.iloc[0].get('complexification_output', {}) # This should contain the profile for iteration 0\n",
        "            if isinstance(source_output_raw, str):\n",
        "                try:\n",
        "                    source_output_dict = json.loads(source_output_raw)\n",
        "                    source_profile = source_output_dict.get('rewritten_text_profile')\n",
        "                except json.JSONDecodeError:\n",
        "                    pass\n",
        "            elif isinstance(source_output_raw, dict):\n",
        "                source_profile = source_output_raw.get('rewritten_text_profile')\n",
        "\n",
        "        if not target_row.empty:\n",
        "            target_output_raw = target_row.iloc[0].get('complexification_output', {}) # This should contain the profile for the last iteration's rewritten text\n",
        "            if isinstance(target_output_raw, str):\n",
        "                try:\n",
        "                    target_output_dict = json.loads(target_output_raw)\n",
        "                    target_profile = target_output_dict.get('rewritten_text_profile')\n",
        "                except json.JSONDecodeError:\n",
        "                    pass\n",
        "            elif isinstance(target_output_raw, dict):\n",
        "                target_profile = target_output_raw.get('rewritten_text_profile')\n",
        "\n",
        "        metric_exceeded_flags = {}\n",
        "        for i, metric_name in enumerate(metrics): # Renamed 'metric' to 'metric_name' to avoid conflict\n",
        "            metric_exceeded_flags[f'{metric_name}_exceeded'] = False\n",
        "\n",
        "            if source_profile and target_profile and len(source_profile) > i and len(target_profile) > i:\n",
        "                src_val = source_profile[i]\n",
        "                tgt_val = target_profile[i]\n",
        "                if isinstance(src_val, (int, float)) and isinstance(tgt_val, (int, float)):\n",
        "                    metric_exceeded_flags[f'{metric_name}_exceeded'] = (tgt_val > src_val)\n",
        "\n",
        "        result_entry = {\n",
        "            'Run ID': unique_run_identifier, # Use unique identifier here\n",
        "            'Original File': original_jsonl_file, # Add original file name\n",
        "            'Iterations': max_iter,\n",
        "            'Cosine Similarity': round(similarity_score, 4),\n",
        "            'Source Text': source_text, # Keep for potential debugging or future use\n",
        "            'Rewritten Text': rewritten_text, # Keep for potential debugging or future use\n",
        "            'Note': 'OK'\n",
        "        }\n",
        "        result_entry.update(metric_exceeded_flags)\n",
        "        results_with_metrics.append(result_entry)\n",
        "\n",
        "    except Exception as e:\n",
        "        error_entry = {\n",
        "            'Run ID': unique_run_identifier, # Use unique identifier here\n",
        "            'Original File': original_jsonl_file, # Add original file name\n",
        "            'Iterations': group['iteration'].max(),\n",
        "            'Cosine Similarity': 0.0,\n",
        "            'Source Text': '',\n",
        "            'Rewritten Text': '',\n",
        "            'Note': f\"Errore: {str(e)}\"\n",
        "        }\n",
        "        for metric_name in metrics:\n",
        "            error_entry[f'{metric_name}_exceeded'] = False\n",
        "        results_with_metrics.append(error_entry)\n",
        "\n",
        "# Converti la lista `results_with_metrics` in un DataFrame pandas\n",
        "metrics_summary_df = pd.DataFrame(results_with_metrics)\n",
        "\n",
        "columns_to_average = ['Cosine Similarity'] + [f'{metric}_exceeded' for metric in metrics]\n",
        "\n",
        "final_summary = metrics_summary_df.groupby('Original File')[columns_to_average].mean().reset_index()\n",
        "\n",
        "print(\"Similarità Cosine Media e Percentuali di Superamento delle Metriche per File:\")\n",
        "\n",
        "# Formattazione per la visualizzazione in Markdown\n",
        "final_summary['Cosine Similarity'] = final_summary['Cosine Similarity'].apply(lambda x: f'{x:.4f}')\n",
        "\n",
        "for metric_name in metrics:\n",
        "    col_name = f'{metric_name}_exceeded'\n",
        "    final_summary[col_name] = final_summary[col_name].apply(lambda x: f'{x * 100:.2f}%')\n",
        "\n",
        "print(final_summary.to_markdown(index=False))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7a5a6dee"
      },
      "source": [
        "analysis_results = []\n",
        "\n",
        "# Convert percentage strings to numeric for comparison\n",
        "analysis_df = final_summary.copy()\n",
        "for col in analysis_df.columns:\n",
        "    if '_exceeded' in col:\n",
        "        analysis_df[col] = analysis_df[col].str.replace('%', '').astype(float)\n",
        "    elif col == 'Cosine Similarity':\n",
        "        analysis_df[col] = analysis_df[col].astype(float)\n",
        "\n",
        "# Analyze Cosine Similarity\n",
        "metric = 'Cosine Similarity'\n",
        "best_model_cs = analysis_df.loc[analysis_df[metric].idxmax()]\n",
        "worst_model_cs = analysis_df.loc[analysis_df[metric].idxmin()]\n",
        "\n",
        "analysis_results.append({\n",
        "    'Metric': metric,\n",
        "    'Best Model': best_model_cs['Original File'],\n",
        "    'Best Value': f\"{best_model_cs[metric]:.4f}\",\n",
        "    'Worst Model': worst_model_cs['Original File'],\n",
        "    'Worst Value': f\"{worst_model_cs[metric]:.4f}\"\n",
        "})\n",
        "\n",
        "# Analyze metric exceedance percentages\n",
        "for metric in metrics: # 'metrics' list is defined in a previous cell\n",
        "    col_name = f'{metric}_exceeded'\n",
        "\n",
        "    # Check if the column exists and has non-null values\n",
        "    if col_name in analysis_df.columns and not analysis_df[col_name].isnull().all():\n",
        "        best_model = analysis_df.loc[analysis_df[col_name].idxmax()]\n",
        "        worst_model = analysis_df.loc[analysis_df[col_name].idxmin()]\n",
        "\n",
        "        analysis_results.append({\n",
        "            'Metric': metric,\n",
        "            'Best Model': best_model['Original File'],\n",
        "            'Best Value': f\"{best_model[col_name]:.2f}%\",\n",
        "            'Worst Model': worst_model['Original File'],\n",
        "            'Worst Value': f\"{worst_model[col_name]:.2f}%\"\n",
        "        })\n",
        "\n",
        "analysis_df_summary = pd.DataFrame(analysis_results)\n",
        "\n",
        "print(\"\\n--- Analisi dei Modelli Migliori e Peggiori per Categoria ---\")\n",
        "print(analysis_df_summary.to_markdown(index=False))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ultimi Salvataggi"
      ],
      "metadata": {
        "id": "hcwqrkDWlwUi"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7dff0080"
      },
      "source": [
        "output_csv_path = 'model_analysis_summary.csv'\n",
        "analysis_df_summary.to_csv(output_csv_path, index=False)\n",
        "\n",
        "print(f\"L'analisi dei modelli è stata salvata con successo in '{output_csv_path}'\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3da02134"
      },
      "source": [
        "output_summary_csv_path = 'metric_summary_per_file.csv'\n",
        "\n",
        "final_summary.to_csv(output_summary_csv_path, index=False)\n",
        "\n",
        "print(f\"Il riepilogo delle metriche per file è stato salvato con successo in '{output_summary_csv_path}'\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cosine Similarity avanzata:"
      ],
      "metadata": {
        "id": "dD4SzuCGpWbV"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "QOzae-lDpaBf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7759b035"
      },
      "source": [
        "# Task\n",
        "Perform a semantic similarity analysis on the rewritten texts and human references using a pre-trained SBERT model. Calculate the cosine similarity for each `run_id`, then analyze the distribution of these scores, and summarize the findings to determine if the text complexification process preserved semantic content. The analysis should include calculating average semantic similarity scores across the dataset and per file, and visualizing the distribution of scores with a clear legend."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "85a0eb2d"
      },
      "source": [
        "## Carica Modello Sentence Embedding\n",
        "\n",
        "### Subtask:\n",
        "Installa le librerie necessarie (ad es. `sentence-transformers` o `transformers` con un modello SBERT) e carica un modello pre-addestrato per la generazione di embedding di frasi.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f161c02e"
      },
      "source": [
        "**Reasoning**:\n",
        "The first step is to install the `sentence-transformers` library as requested in the instructions.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c75d1bfc"
      },
      "source": [
        "import sys\n",
        "!{sys.executable} -m pip install sentence-transformers\n",
        "\n",
        "print(\"Libreria 'sentence-transformers' installata con successo.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3857201c"
      },
      "source": [
        "**Reasoning**:\n",
        "The previous step installed the `sentence-transformers` library. The next instruction is to load a pre-trained Sentence-BERT model, specifically 'all-MiniLM-L6-v2', for generating sentence embeddings.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ffd2fb9e"
      },
      "source": [
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "# Carica un modello pre-addestrato di Sentence-BERT\n",
        "model_name = 'all-MiniLM-L6-v2'\n",
        "model = SentenceTransformer(model_name)\n",
        "\n",
        "print(f\"Modello SBERT '{model_name}' caricato con successo.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0b0eb161"
      },
      "source": [
        "import numpy as np\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "semantic_results = []\n",
        "\n",
        "for (original_jsonl_file, run_id_in_file), group in df.groupby(['original_jsonl_file', 'run_id']):\n",
        "    unique_run_identifier = f\"{original_jsonl_file}_{run_id_in_file}\"\n",
        "\n",
        "    try:\n",
        "        # Recupera Rewritten Text (dall'ultima iterazione disponibile)\n",
        "        max_iter = group['iteration'].max()\n",
        "        target_row = group[group['iteration'] == max_iter]\n",
        "\n",
        "        rewritten_text = \"\"\n",
        "        if not target_row.empty:\n",
        "            complexification_output_raw = target_row.iloc[0].get('complexification_output', '')\n",
        "\n",
        "            if isinstance(complexification_output_raw, str):\n",
        "                try:\n",
        "                    complexification_output_dict = json.loads(complexification_output_raw)\n",
        "                    rewritten_text = complexification_output_dict.get('rewritten_text', '')\n",
        "                except json.JSONDecodeError:\n",
        "                    pass # If not valid JSON, rewritten_text remains empty\n",
        "            elif isinstance(complexification_output_raw, dict):\n",
        "                rewritten_text = complexification_output_raw.get('rewritten_text', '')\n",
        "\n",
        "        # Recupera Human Reference (dalla prima iterazione, o da qualsiasi riga per il run_id)\n",
        "        # Assuming human_reference is consistent across iterations for a given run_id\n",
        "        human_reference_row = group.iloc[0]\n",
        "        human_reference = human_reference_row.get('human_reference', '')\n",
        "\n",
        "        # Pulizia e verifica (Gestione NaN di Pandas e stringhe vuote)\n",
        "        if pd.isna(rewritten_text): rewritten_text = \"\"\n",
        "        if pd.isna(human_reference): human_reference = \"\"\n",
        "\n",
        "        semantic_similarity_score = 0.0\n",
        "        note = 'OK'\n",
        "\n",
        "        # Check if both texts are non-empty after cleaning\n",
        "        if not rewritten_text.strip() or not human_reference.strip():\n",
        "            note = f\"Testo mancante (Rewritten: {bool(rewritten_text.strip())}, Human Ref: {bool(human_reference.strip())})\"\n",
        "        else:\n",
        "            # Genera embeddings per i testi\n",
        "            embeddings = model.encode([rewritten_text, human_reference])\n",
        "\n",
        "            # Calcola la similarità coseno tra gli embeddings\n",
        "            # Reshape per sklearn.metrics.pairwise.cosine_similarity\n",
        "            semantic_similarity_score = cosine_similarity(embeddings[0].reshape(1, -1), embeddings[1].reshape(1, -1))[0][0]\n",
        "\n",
        "        semantic_results.append({\n",
        "            'Run ID': unique_run_identifier,\n",
        "            'Original File': original_jsonl_file,\n",
        "            'Semantic Cosine Similarity': round(semantic_similarity_score, 4),\n",
        "            'Note': note\n",
        "        })\n",
        "\n",
        "    except Exception as e:\n",
        "        semantic_results.append({\n",
        "            'Run ID': unique_run_identifier,\n",
        "            'Original File': original_jsonl_file,\n",
        "            'Semantic Cosine Similarity': 0.0,\n",
        "            'Note': f\"Errore: {str(e)}\"\n",
        "        })\n",
        "\n",
        "semantic_df = pd.DataFrame(semantic_results)\n",
        "\n",
        "print(\"Prime 5 righe del DataFrame con la similarità semantica:\")\n",
        "display(semantic_df.head())\n",
        "\n",
        "# Calcola la media della similarità semantica per ogni file originale\n",
        "average_semantic_similarity_per_file = semantic_df.groupby('Original File')['Semantic Cosine Similarity'].mean().reset_index()\n",
        "\n",
        "print(\"\\nSimilarità Cosine Semantica Media per File:\")\n",
        "print(average_semantic_similarity_per_file.to_markdown(index=False))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f59dae21"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "plt.figure(figsize=(12, 7))\n",
        "sns.histplot(semantic_df['Semantic Cosine Similarity'], kde=True, bins=20, color='skyblue')\n",
        "plt.title('Distribuzione della Similarità Cosine Semantica')\n",
        "plt.xlabel('Similarità Cosine Semantica')\n",
        "plt.ylabel('Frequenza')\n",
        "plt.grid(axis='y', alpha=0.75)\n",
        "plt.show()\n",
        "\n",
        "# Visualize distribution per original file (e.g., using boxplots or KDE plots per group)\n",
        "plt.figure(figsize=(14, 8))\n",
        "sns.boxplot(x='Original File', y='Semantic Cosine Similarity', data=semantic_df)\n",
        "plt.xticks(rotation=45, ha='right')\n",
        "plt.title('Distribuzione della Similarità Cosine Semantica per File Originale')\n",
        "plt.xlabel('File Originale')\n",
        "plt.ylabel('Similarità Cosine Semantica')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"Visualizzazione della distribuzione della Similarità Cosine Semantica completata.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2c102d2a"
      },
      "source": [
        "**Reasoning**:\n",
        "The semantic similarity analysis and its interpretation have been completed. The final step is to save the detailed semantic similarity results to a CSV file for record-keeping and potential future analysis, as the task implies summarizing and saving findings.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "003e42fc"
      },
      "source": [
        "output_semantic_results_csv_path = 'detailed_semantic_similarity_results.csv'\n",
        "semantic_df.to_csv(output_semantic_results_csv_path, index=False)\n",
        "\n",
        "print(f\"I risultati dettagliati della similarità semantica sono stati salvati con successo in '{output_semantic_results_csv_path}'\")"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}